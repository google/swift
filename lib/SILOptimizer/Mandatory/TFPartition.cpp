//===--- TFPartition.cpp - Split Tensor ops out of mainline flow ----------===//
//
// This source file is part of the Swift.org open source project
//
// Copyright (c) 2014 - 2017 Apple Inc. and the Swift project authors
// Licensed under Apache License v2.0 with Runtime Library Exception
//
// See https://swift.org/LICENSE.txt for license information
// See https://swift.org/CONTRIBUTORS.txt for the list of Swift project authors
//
//===----------------------------------------------------------------------===//
//
// This pass splits tensor operations out into seperate functions - one per
// TensorFlow graph that is generated by the TFLowerGraph functionality.
//
//===----------------------------------------------------------------------===//

#define DEBUG_TYPE "tf-partition"
#include "TFUtilities.h"
#include "swift/SILOptimizer/PassManager/Passes.h"
#include "swift/SILOptimizer/PassManager/Transforms.h"
#include "swift/SILOptimizer/Analysis/DominanceAnalysis.h"
#include "swift/SIL/CFG.h"
#include "swift/SIL/DebugUtils.h"
#include "swift/SIL/SILArgument.h"
#include "swift/SIL/SILCloner.h"
#include "swift/AST/DiagnosticsSIL.h"
#include "swift/AST/Expr.h"
#include "llvm/ADT/DepthFirstIterator.h"
#undef DEBUG_TYPE
#include "llvm/Support/GenericDomTreeConstruction.h"
#define DEBUG_TYPE "tf-partition"
using namespace swift;
using namespace tf;


template<typename...T, typename...U>
static InFlightDiagnostic
diagnose(ASTContext &Context, SourceLoc loc, Diag<T...> diag, U &&...args) {
  return Context.Diags.diagnose(loc, diag, std::forward<U>(args)...);
}

/// Returns true if the partitioning pass should ignore this user.
static bool isUserIgnoredByPartitioning(SILInstruction *inst) {
  return isa<DebugValueInst>(inst) || isa<RefCountingInst>(inst);
}

/// Given a decl for a struct that has a single field (typically because it is
/// known to be a standard library type like Int or Float), return the canonical
/// type of the single member, asserting and aborting if we get something
/// unexpected.
static CanType getSingleElementDeclFieldType(NominalTypeDecl *decl) {
  auto fieldIt = decl->getStoredProperties().begin();
  assert(fieldIt != decl->getStoredProperties().end() &&
         "Struct should have one member");
  auto fieldType = (*fieldIt++)->getType()->getCanonicalType();
  assert(fieldIt == decl->getStoredProperties().end() &&
         "Struct should have one member");
  return fieldType;
}

static bool isOverflowCheckingIntegerOp(SILInstruction *inst) {
  auto *BI = dyn_cast<BuiltinInst>(inst);
  if (!BI) return false;

  switch (BI->getBuiltinInfo().ID) {
  default: return false;
  case BuiltinValueKind::UAddOver:
  case BuiltinValueKind::SAddOver:
  case BuiltinValueKind::USubOver:
  case BuiltinValueKind::SSubOver:
  case BuiltinValueKind::UMulOver:
  case BuiltinValueKind::SMulOver:
    return true;
  }
}

/// Given an overflow-checking integer operation, return true if the overflow
/// result will be unused in the tensor program.  This could be because the
/// overflow result is already dead, because it is extracted but the extract
/// isn't used, or if the result is only used by code that provably won't make
/// it into the tensor program.
static bool canMoveOverflowCheckingInstToTensorProgram(BuiltinInst *BI) {
  // Annoyingly, these builtins are modeled as returning a tuple, which then
  // has tuple extracts hanging off of it.
  for (auto *use : BI->getUses()) {
    auto *extract = dyn_cast<TupleExtractInst>(use->getUser());
    if (!extract) return false;

    // If this is a use of the normal result of the call, then we can ignore
    // it - this can be moved to the tensor program.
    if (extract->getFieldNo() == 0)
      continue;
    assert(extract->getFieldNo() == 1 && "Overflowing ops only have 2 results");

    // Check all uses of the TupleExtract.  If any of them are going to end up
    // in the tensor program, then we cannot move it.
    for (auto *overflowUse : extract->getUses()) {
      auto overflowUser = overflowUse->getUser();
      // CondFail won't get moved over.
      if (isa<CondFailInst>(overflowUser))
        continue;
      return false;
    }
  }
  return true;
}


/// If the specified scalar operation can be partitioned and run on the
/// accelerator, return the name of the op to use to implement it.
static std::string getPartitionedScalarOpName(SILInstruction *I) {
  // We can turn integer and FP literals into constant nodes if their type is
  // compatible.
  if (isa<IntegerLiteralInst>(I) || isa<FloatLiteralInst>(I)) {
    auto resultTy = I->getResults()[0]->getType();
    if (isValidTensorFlowElementType(resultTy.getSwiftRValueType()))
      return "__tfop_Const,cd:t";
  }

  auto *BI = dyn_cast<BuiltinInst>(I);
  if (!BI) return std::string();

  // These are true if the first operand is int, fp, etc, not counting vectors
  // or other exotic types.  It is also only true if it is a type we can
  // represent in Tensor elements.
  bool isInt = false, isFP = false;
  if (BI->getNumOperands() != 0) {
    auto opTy = BI->getOperand(0)->getType();
    if (isValidTensorFlowElementType(opTy.getSwiftRValueType())) {
      isInt = opTy.is<BuiltinIntegerType>();
      isFP = opTy.is<BuiltinFloatType>();
    }
  }

  // Perform a final boolean validity check and add the "(t,t)->t" signature
  // for a tensor op.
  auto check = [&](bool cond, StringRef result) -> std::string {
    if (!cond) return std::string();
    return "__tfop_" + result.str() + ",tt:t";
  };

  // Given an overflowing operation, return true if the overflow result will be
  // dead in the tensor program.
  auto overflowDead = [&]() -> bool {
    return canMoveOverflowCheckingInstToTensorProgram(BI);
  };

  switch (BI->getBuiltinInfo().ID) {
  default: return StringRef();
  // TODO: FP Comparisons.  Unsigned ops, which need int->uint casts.
  case BuiltinValueKind::ICMP_EQ: return check(isInt, "Equal");
  case BuiltinValueKind::ICMP_NE: return check(isInt, "NotEqual");
  case BuiltinValueKind::ICMP_SLT: return check(isInt, "Less");
  case BuiltinValueKind::ICMP_SGT: return check(isInt, "Greater");
//case BuiltinValueKind::ICMP_UGT: return check(isInt, "Greater");
  case BuiltinValueKind::ICMP_SLE: return check(isInt, "LessEqual");
//case BuiltinValueKind::ICMP_ULE: return check(isInt, "LessEqual");
  case BuiltinValueKind::ICMP_SGE: return check(isInt, "GreaterEqual");
//case BuiltinValueKind::ICMP_UGE: return check(isInt, "GreaterEqual");
  case BuiltinValueKind::Add: return check(isInt, "Add");
  case BuiltinValueKind::Sub: return check(isInt, "Sub");
  case BuiltinValueKind::Mul: return check(isInt, "Mul");
  case BuiltinValueKind::FAdd: return check(isFP, "Add");
  case BuiltinValueKind::FSub: return check(isFP, "Sub");
  case BuiltinValueKind::FMul: return check(isFP, "Mul");
  //case BuiltinValueKind::UAddOver:
  //case BuiltinValueKind::USubOver:
  //case BuiltinValueKind::UMulOver:
  case BuiltinValueKind::SAddOver: return check(isInt&overflowDead(), "Add");
  case BuiltinValueKind::SSubOver: return check(isInt&overflowDead(), "Sub");
  case BuiltinValueKind::SMulOver: return check(isInt&overflowDead(), "Mul");
  }
}

/// If the specified type is a TensorHandle<T> type, return it.  Otherwise, it
/// must be a primitive type T.  In that case, wrap it to form TensorHandle<T>.
static SILType convertToTensorHandleType(SILType ty) {
  // If this is already TensorHandle<T>, return it.
  if (isTensorHandle(ty.getSwiftRValueType()))
    return ty;

  // Otherwise, this is a valid TensorFlow element type "T", like Builtin.Int32,
  // turn it into a TensorHandle<T> type.
  assert(isValidTensorFlowElementType(ty.getSwiftRValueType()));
  auto decl = ty.getASTContext().getTensorHandleDecl();
  auto tensorType =
    BoundGenericClassType::get(decl, /*parent*/Type(),
                               ty.getSwiftRValueType());

  return SILType::getPrimitiveObjectType(tensorType->getCanonicalType());
}



//===----------------------------------------------------------------------===//
//                  BlocksReachingTensorCode CFG Subset
//===----------------------------------------------------------------------===//

/// These nodes mirrors a CFG subset of the function being partitioned, which
/// makes it easy to reuse the dominator algorithms in LLVM.  While it seems
/// silly to make a clone of a graph to produce a filtered view on it, getting
/// the right view projecting iterators to work is more complicated than it is
/// worth at this point.
///
/// As such, this is a very simple representation, which can be optimized later
/// if it ever becomes a performance problem (doubtful).
///
namespace {
class BlocksReachingTensorCode;
struct SILBBSubsetNode {
  // We care about the addresses of these nodes, so disable these to avoid
  // accidental copies.
  SILBBSubsetNode() = delete;
  SILBBSubsetNode(const SILBBSubsetNode&) = delete;
public:
  SILBasicBlock *BB;
  BlocksReachingTensorCode *Parent;
  SILBBSubsetNode(SILBasicBlock *BB, BlocksReachingTensorCode *Parent)
    : BB(BB), Parent(Parent) {}

  SILBBSubsetNode(SILBBSubsetNode &&rhs) {
    BB = rhs.BB;
    Parent = rhs.Parent;
  }

  // These are predecessors and successors of the CFG subset.
  typedef std::vector<SILBBSubsetNode*> BBListTy;
  BBListTy Predecessors, Successors;


  // Requirements of the domtree implementation.
  BlocksReachingTensorCode *getParent() const { return Parent; }
  void printAsOperand(llvm::raw_ostream &O, bool printType = true) {
    BB->printAsOperand(O, printType);
  }
};
} // end anonymous namespace


namespace llvm {
  template <> struct GraphTraits<SILBBSubsetNode*> {
    using ChildIteratorType = SILBBSubsetNode::BBListTy::iterator;
    using Node = SILBBSubsetNode;
    using NodeRef = SILBBSubsetNode*;

    static ChildIteratorType child_begin(NodeRef N) {
      return N->Successors.begin();
    }
    static ChildIteratorType child_end(NodeRef N) {
      return N->Successors.end();
    }
  };

  template <> struct GraphTraits<Inverse<SILBBSubsetNode*>> {
    using ChildIteratorType = SILBBSubsetNode::BBListTy::iterator;
    using Node = SILBBSubsetNode;
    using NodeRef = SILBBSubsetNode*;
    static ChildIteratorType child_begin(NodeRef N) {
      return N->Predecessors.begin();
    }
    static ChildIteratorType child_end(NodeRef N) {
      return N->Predecessors.end();
    }
  };
} // end namespace llvm



namespace {
/// Represent a subset of a function in a way that we can fulfill the model of
/// the LLVM Graph abstractions, allowing us to get post-dominators for a subset
/// of the nodes and edges in a function.
///
/// A Swift function that contains tensor operations will often have a number
/// of blocks "hanging off it" that represent failure paths: integer overflow
/// checks, precondition failures, etc.  Also, the function may have a
/// significant amount of general computation that happens after the meat of
/// tensor computation happens.
///
/// Unfortunately, these exits blocks in particular will generally pessimize
/// PostDominator information, because the extraneous edges go to blocks that
/// end with unreachable.  This means that we frequently get degenerate blocks
/// where the only post dominator of two related blocks is the exit node for
/// the function, which unifies the blocks that end with 'return' and the
/// blocks that end with 'unreachable'.
///
/// To solve this, we compute the set of blocks that we need to represent in
/// the tensor slice because they can reach tensor computation or a direct use
/// of that computation.  This subset of the function is what we need to
/// generate accurate post dominator info.
///
class BlocksReachingTensorCode {
  friend struct llvm::GraphTraits<BlocksReachingTensorCode*>;
  friend struct llvm::GraphTraits<llvm::Inverse<BlocksReachingTensorCode*>>;

  /// The function this slice is a subset of.
  SILFunction &fn;

  /// These are all of the nodes themselves.
  std::vector<SILBBSubsetNode> nodes;

  /// This map contains all of the SILBBSubsetNode's that make up the subset
  /// graph.
  llvm::DenseMap<SILBasicBlock*, SILBBSubsetNode*> nodeMap;

  /// This is the post dominator tree built over our node subset.
  llvm::DominatorTreeBase<SILBBSubsetNode, true> PDI;
public:
  BlocksReachingTensorCode(SILFunction &fn) : fn(fn) {}
  void compute(ArrayRef<SILInstruction*> ops);

  SILBBSubsetNode *getNode(SILBasicBlock *BB) const {
    auto i = nodeMap.find(BB);
    assert(i != nodeMap.end() && i->second && "BasicBlock not in our subset");
    return i->second;
  }

  SILBBSubsetNode *getEntryBlock() const {
    return getNode(fn.getEntryBlock());
  }

  /// Return true if the specified block is in our subset of the function.
  bool contains(SILBasicBlock *bb) const {
    return nodeMap.count(bb);
  }

  bool postDominates(SILBasicBlock *dominator, SILBasicBlock *dominatee) {
    return PDI.dominates(getNode(dominator), getNode(dominatee));
  }

  SILBasicBlock *findNearestCommonPostDominator(SILBasicBlock *B1,
                                                SILBasicBlock *B2) {
    auto res = PDI.findNearestCommonDominator(getNode(B1), getNode(B2));
    return res ? res->BB : nullptr;
  }


  SILBasicBlock *getPostIDom(SILBasicBlock *BB) {
    auto PDINode = PDI[getNode(BB)]->getIDom();
    return PDINode ? PDINode->getBlock()->BB : nullptr;
  }

  void dump();

public:
  // Random stuff used by DominatorTree internals.  Don't use generally.
  SILBBSubsetNode &front() const { return *getEntryBlock(); }
};
} // end anonymous namespace

/// \brief An iterator type that allows iterating over the address of the
/// elements returned via some other iterator.
///
/// \code
///   using iterator = address_iterator<SmallVectorImpl<T>::iterator>;
/// \endcode
template <typename WrappedIteratorT,
          typename T = decltype(&*std::declval<WrappedIteratorT>())>
struct address_iterator
  : llvm::iterator_adaptor_base<
          address_iterator<WrappedIteratorT>, WrappedIteratorT,
          typename std::iterator_traits<WrappedIteratorT>::iterator_category,
          T> {
  address_iterator() = default;
  template <typename U>
  address_iterator(U &&u)
      : address_iterator::iterator_adaptor_base(std::forward<U &&>(u)) {}

  T operator*() const { return &*this->I; }
};

namespace llvm {
  template<>
  struct GraphTraits<BlocksReachingTensorCode*>
                            : public GraphTraits<SILBBSubsetNode*> {
    using GraphType = BlocksReachingTensorCode*;
    using NodeRef = SILBBSubsetNode*;

    static NodeRef getEntryNode(GraphType F) {
      return F->getEntryBlock();
    }

    typedef address_iterator<std::vector<SILBBSubsetNode>::iterator>
                               nodes_iterator;
    static nodes_iterator nodes_begin(GraphType F) {
      return nodes_iterator(F->nodes.begin());
    }
    static nodes_iterator nodes_end(GraphType F) {
      return nodes_iterator(F->nodes.end());
    }

    static unsigned size(GraphType F) { return F->nodes.size(); }
  };

  template<> struct GraphTraits<Inverse<BlocksReachingTensorCode*>>
  : public GraphTraits<Inverse<swift::SILBasicBlock*>> {
    using GraphType = Inverse<BlocksReachingTensorCode*>;
    using NodeRef = SILBBSubsetNode*;

    typedef address_iterator<std::vector<SILBBSubsetNode>::iterator>
                              nodes_iterator;
    static nodes_iterator nodes_begin(GraphType F) {
      return nodes_iterator(F.Graph->nodes.begin());
    }
    static nodes_iterator nodes_end(GraphType F) {
      return nodes_iterator(F.Graph->nodes.end());
    }
    static unsigned size(GraphType F) { return F.Graph->nodes.size(); }
  };
} // end namespace llvm

// This template requires explicit instantiation.
template class llvm::DominatorTreeBase<SILBBSubsetNode, true>;

/// Compute the set of blocks that can reach the specified operations, and
/// uses of them.
void BlocksReachingTensorCode::compute(ArrayRef<SILInstruction*> ops) {
  assert(!ops.empty() && "Cannot compute empty CFG subset");
  SmallVector<SILBasicBlock*, 8> worklist;

  // We seed the worklist with the blocks that the operations occur in, along
  // with the blocks containing all uses of the operands.  These uses may not
  // themselves be tensor results that we partition: but they may be the send
  // operations that cause copy out or function results.
  for (auto *i : ops) {
    auto instBB = i->getParent();
    worklist.push_back(instBB);

    // Add the blocks that any users live in.
    for (auto result : i->getResults()) {
      for (auto user : result->getUses()) {
        if (user->getUser()->getParent() != instBB &&
            !isUserIgnoredByPartitioning(user->getUser()))
          worklist.push_back(user->getUser()->getParent());
      }
    }
  }

  // Make sure the nodes are never reallocated out from under us.
  nodes.reserve(fn.getBlocks().size());

  // Figure out all of the blocks that should be included.
  while (!worklist.empty()) {
    auto *bb = worklist.pop_back_val();

    // If we already visited this block, we're done.  Otherwise insert it,
    // creating the SILBBSubsetNode for this BB.
    auto &entry = nodeMap[bb];
    if (entry != nullptr) continue;

    nodes.emplace_back(SILBBSubsetNode(bb, this));
    entry = &nodes.back();

    // Its predecessors can also reach a tensor op.
    worklist.append(bb->pred_begin(), bb->pred_end());
  }

  assert(nodeMap.count(fn.getEntryBlock()) &&
         "Entry block should be reachable from Tensor work");

  // Now that all of the nodes are created, we can wire up the predecessor and
  // successor lists.
  for (auto &node : nodes) {
    for (auto *succ : node.BB->getSuccessorBlocks()) {
      auto it = nodeMap.find(succ);
      if (it != nodeMap.end())
        node.Successors.push_back(it->second);
    }
    for (auto *pred : node.BB->getPredecessorBlocks()) {
      auto it = nodeMap.find(pred);
      if (it != nodeMap.end())
        node.Predecessors.push_back(it->second);
    }
  }

  // Now that we have our CFG subset, compute the post dominator tree from it.
  PDI.recalculate(*this);
}

void BlocksReachingTensorCode::dump() {
  PDI.print(llvm::errs());
}


//===----------------------------------------------------------------------===//
//                             FunctionPartitioner
//===----------------------------------------------------------------------===//

namespace {
/// Marking values in the host program need to either be moved, copied, or have
/// their results sent over to the accelerator.
enum class Marking {
  Copy,      // This instruction is run on both the host and accelerator.
  Move,      // This instruction is run on the accelerator, not the host.
  Send,      // The value produced by this instruction is copied to accelerator.
  Argument,  // The value is passed as an argument to the tensor function.
  Delete,    // This instruction is simply deleted (e.g. debug_value)
};

class TFFunctionPartition {
public:
  SILFunction &fn;
  ModuleDecl &tensorFlowModule;  // TensorFlow standard library.
  DominanceInfo &DI;
  BlocksReachingTensorCode tensorCodeBlocks;

  /// These are all the tensor ops found in the initial scan over the function.
  SmallPtrSet<SILInstruction*, 8> tensorOpsSet;

  /// This keeps track of the set of blocks that are marked as needing to be
  /// partitioned out to the accelerator.  If the block is in this set, then
  /// some instruction in the block has to run on the accelerator.
  SmallPtrSet<SILBasicBlock*, 8> markedBlocks;

  /// This contains a set of all basic blocks that are immediate successors of
  /// TensorOp blocks, but which are outside of the tensor program (typically
  /// these are error edges).  These blocks need to kill the tensor program if
  /// reached.
  SmallPtrSet<SILBasicBlock*, 8> tensorKillBlocks;

  /// As the primary tensor operations are marked, the nearest common ancestor
  /// (NCA) in the dominator tree of the tensor operations is found.  This will
  /// be the entry block of the tensor computation, and marks the point where
  /// the tensor computation is started (and where arguments are passed).
  ///
  /// Among other things, this trims off the front matter that often ends up at
  /// the beginning of functions.
  ///
  /// The instruction pointed to here is either the first Tensor operation (if
  /// there is one which dominates all other ops) or the terminator of the block
  /// that dominates all of the operations.
  SILInstruction *tensorStartPoint = nullptr;

  /// Similar to the start point, this indicates the first instruction after
  /// the last Tensor operation, which is when the computation should be
  /// completed and results are returned.  If there are no tensor ops in the
  /// final block, then this will be the entry instruction.
  SILInstruction *tensorEndPoint = nullptr;

  /// The values passed as arguments to the tensor function.
  SmallVector<SILValue, 4> tensorFnArguments;

  /// The instructions that are to be run on the accelerator.
  llvm::DenseMap<SILInstruction*, Marking> markedInstructions;

  /// BB Arguments that are marked as being moved or copied over.  If a marked
  /// argument is moved over, it is deleted from the host program.  If the
  /// host also uses the argument, then a copy will have to be inserted back
  /// from the accelerator to the host.
  llvm::DenseMap<SILArgument*, Marking> markedBBArguments;

  /// The set of values that must be sent to the accelerator.
  SmallPtrSet<SILValue, 8> valuesToSend;

  /// Set of all of the __tf_send calls that silence copy-in warnings.
  SmallPtrSet<SILInstruction*, 8> explicitCopyMarkers;

  /// These are the results of tensor values that should be returned by the
  /// extracted function.
  SmallVector<SILValue, 4> resultValues;
public:
  TFFunctionPartition(SILFunction &Fn, SILPassManager *PM,
                      ModuleDecl &tensorFlowModule)
    : fn(Fn), tensorFlowModule(tensorFlowModule),
      DI(*PM->getAnalysis<DominanceAnalysis>()->get(&Fn)),
      tensorCodeBlocks(Fn) {
  }

  bool markFunction();

  struct PartitionedTensorProgram {
    SILFunction *fn;  // The function representing the tensor program.

    /// These are placeholder instructions inserted during partitioning to
    /// represent the tensor program itself.  These will be replaced when the
    /// tensor function is lowered to a TF graph.
    StringLiteralInst *programPlaceholder;
    IntegerLiteralInst *programLengthPlaceholder;
    StringLiteralInst *entryFunctionNamePlaceholder;

    /// This is the "TensorFlow.TensorComputation" object returned by the
    /// '_swift_tfc_StartTensorComputation' runtime API entrypoint.  This is
    /// returned as null if the tensorflow module is invalid and no
    /// transformation has been made.
    SILValue theTensorComputation;
  };
  PartitionedTensorProgram partition();

  void diagnoseCopyInIfNotSend(SILValue value, SILInstruction *user);
  bool diagnoseCopyOutIfNotReceive(SILValue value, SILInstruction *user);

private:
  // Marking.
  void markBlock(SILBasicBlock *BB);
  void markInstruction(SILInstruction &inst, Marking mark);
  void markArgument(SILArgument *arg, SILInstruction *user);
  void markValue(SILValue value, SILInstruction *user);

  bool sinkValueIntoRegionForPromotion(SILInstruction *inst);

  // Rewriting the host function.
  PartitionedTensorProgram insertTensorComputationStartEndTerminate();
};
} // end anonymous namespace

/// Check to see if the specified value being copied into a partition for the
/// accelerator is our designated "send" operation.  If so, we're fine,
/// otherwise emit a warning to tell the programmer that they are doing
/// something that induces an implicit data transfer into their code.
void TFFunctionPartition::diagnoseCopyInIfNotSend(SILValue value,
                                                  SILInstruction *user) {
  // If it isn't the result of a "send" operation, then produce a warning about
  // an implicit copy to the accelerator.
  if (auto *apply = dyn_cast<ApplyInst>((SILNode*)value))
    if (auto *callee = dyn_cast<FunctionRefInst>(apply->getCallee()))
      if (callee->getReferencedFunction()->getName().contains("__tf_send")) {
        explicitCopyMarkers.insert(apply);
        return;
      }

  // If we have a struct extract from a type like Int or Float, look through it
  // to the Int or Float value itself, which will have better source location
  // information.  The struct-extract came from the implementation of some
  // operator in the standard library like "+", and we want the source of the
  // parameter.
  if (auto *sei = dyn_cast<StructExtractInst>((SILNode*)value))
    if (sei->getType().getSwiftRValueType()->is<BuiltinType>())
      value = sei->getOperand();

  // Try to determine a good source location to report.
  auto loc = getUserSourceLocation(value.getDebugLocation());

  // Try to make a useful description of the value being copied to help
  // disambiguate.
  std::string description = "value";
  if (auto expr = loc.getAsASTNode<Expr>()) {
    if (auto *ae = dyn_cast<ApplyExpr>(expr->getSemanticsProvidingExpr())) {
      if (isa<ConstructorRefCallExpr>(ae->getFn()))
        description = "'" + expr->getType()->getString() + "'";
      else if (isa<DotSyntaxCallExpr>(ae->getFn()))
        description = "method result";
    }
  } else if (auto decl = loc.getAsASTNode<Decl>()) {
    if (auto pd = dyn_cast<ParamDecl>(decl))
      description = "'" + pd->getName().str().str() + "'";
  }

  // Emit the warning.
  diagnose(fn.getModule().getASTContext(), loc.getSourceLoc(),
           diag::tf_value_implicitly_copied_to_accel, description)
    .highlight(loc.getSourceRange());

  // If the use is on a different line, emit a note showing where it is.
  auto userLoc = getUserSourceLocation(user->getDebugLocation());
  auto &SM = fn.getModule().getASTContext().SourceMgr;

  if (SM.findBufferContainingLoc(loc.getSourceLoc()) !=
        SM.findBufferContainingLoc(userLoc.getSourceLoc()) ||
      SM.getLineNumber(loc.getSourceLoc()) !=
        SM.getLineNumber(userLoc.getSourceLoc())) {
    diagnose(fn.getModule().getASTContext(), userLoc.getSourceLoc(),
             diag::tf_value_implicitly_copied_to_host_computed_used_here)
    .highlight(userLoc.getSourceRange());
  }
}

bool TFFunctionPartition::
diagnoseCopyOutIfNotReceive(SILValue value, SILInstruction *user) {
  /// Check to see if the specified value being copied into a partition for the
  /// accelerator is our designated "receive" operation.  If so, we're fine,
  /// otherwise emit a warning to tell the programmer that they are doing
  /// something that induces an implicit data transfer into their code.

  // If it isn't the result of a "send" operation, then produce a warning about
  // an implicit copy to the accelerator.
  if (auto *apply = dyn_cast<ApplyInst>(user))
    if (auto *fn = dyn_cast<FunctionRefInst>(apply->getCallee()))
      if (fn->getReferencedFunction()->getName().contains("__tf_receive")) {
        explicitCopyMarkers.insert(apply);
        return false;
      }

  auto &ctx = fn.getModule().getASTContext();

  // Try to determine a good source location to report.
  auto loc = getUserSourceLocation(value.getDebugLocation());

  // Emit the warning.
  diagnose(ctx, loc.getSourceLoc(), diag::tf_value_implicitly_copied_to_host)
    .highlight(loc.getSourceRange());

  // If the use is at a different position, emit a note showing where it is.
  auto userLoc = getUserSourceLocation(user->getDebugLocation());
  if (loc.getSourceLoc() != userLoc.getSourceLoc()) {
    diagnose(ctx, userLoc.getSourceLoc(),
             diag::tf_value_implicitly_copied_to_host_computed_used_here)
    .highlight(userLoc.getSourceRange());
  }
  return true;
}



/// Some instruction in the specified block needs to be split out to the
/// accelerator, so we mark it (and its control dependencies) as to-be-moved
/// over.
void TFFunctionPartition::markBlock(SILBasicBlock *BB) {
  // "tensorStartPoint" marks the start of the extracted function, so it must
  // dominate all blocks that are to be extracted.
  assert(DI.dominates(tensorStartPoint->getParent(), BB) &&
         "Marking instructions out of the tensor region?");

  // Insert the block into our set - if the block is already there, we have
  // nothing more to do.
  if (!markedBlocks.insert(BB).second)
    return;

  // Walk predecessors until we find marked blocks or other blocks we are
  // control-dependent on.  We only scan the region post dominated by BB.
  //
  // Note that though this is bounded, that it isn't a very efficient algorithm
  // since each block marking can walk the entire function's CFG, but it is good
  // enough for now.  It would probably make more sense to walk the pdom tree
  // instead of walking the CFG.
  SmallVector<SILBasicBlock*, 8> worklist;
  worklist.push_back(BB);

  // The visited set keeps track of blocks that have been added to the worklist,
  // to ensure we don't process something more than once.
  SmallPtrSet<SILBasicBlock*, 32> visited;
  visited.insert(BB);

  // Walk up the CFG looking for terminators we are control-dependent on.
  while (!worklist.empty()) {
    auto thisBB = worklist.pop_back_val();
    assert(tensorCodeBlocks.postDominates(BB, thisBB) &&
           "Should only be scanning the region pdom'd by BB");

    // If we found the start of the region we are extracting, then stop there.
    if (thisBB == tensorStartPoint->getParent())
      continue;

    // Check the predecessors of this block.  If any of them have multiple
    // successors, then we may be control-dependent on that conditional.
    for (auto pred : thisBB->getPredecessorBlocks()) {
      // Count the number of successors of this block which are tensor related.
      // If we see successors that are not tensor related, we'll remember that
      // so we can insert a kill of the tensor program.
      unsigned numTensorSuccs = 0;
      for (auto succ : pred->getSuccessorBlocks()) {
        if (tensorCodeBlocks.contains(succ))
          ++numTensorSuccs;
        else {
          assert(succ->getSinglePredecessorBlock() &&
                 "Need to split critical edges??");
          tensorKillBlocks.insert(succ);
        }
      }

      // Check to see if pred is already visited or if its terminator is already
      // marked then we don't need to reprocess it.
      if (visited.count(pred) ||
          markedInstructions.count(pred->getTerminator()))
        continue;

      // If the predecessor has a single tensor-related successor (us) then we
      // aren't control-dependent on it.  It may be control-dependent on
      // something though.
      if (numTensorSuccs == 1) {
        worklist.push_back(pred);
        continue;
      }

      // Otherwise, check to see if BB is post-dominated by thisBB, but not by
      // pred - in other words that it is the post dominance frontier for BB.
      // If so, that means that the terminator in pred controls whether BB is
      // executed, so it must be marked.
      if (!tensorCodeBlocks.postDominates(BB, pred)) {
        auto predTerm = pred->getTerminator();
        if (isa<BranchInst>(predTerm) || isa<CondBranchInst>(predTerm)) {
          markInstruction(*predTerm, Marking::Copy);
          continue;
        }

        predTerm->dump();
        assert(0 && "FIXME: Handle non-branch terminators like try_apply, which"
               "cannot (in general) be moved to the accelerator");
      }

      // Otherwise, the predecessor is just another block post-dominated by BB,
      // continue walking it.
      worklist.push_back(pred);
      visited.insert(pred);
    }
  }
}


/// When considering whether we should promote a scalar operation to a tensor
/// op in the graph, we have several cases.
namespace {
enum class ScalarPromoteClass {
  NeverPromote,  ///< Do not promote this operation.
  CanPromote,    ///< We can promote this operation, but an argument is also ok.
  ShouldPromote, ///< This is cheaper to run in graph than on the host.
};
} // end anonymous namespace.

/// Determine whether we can promote the specified scalar instruction to a
/// tensor operation in the graph.
static ScalarPromoteClass shouldPromoteToTensorOp(SILInstruction *inst) {
  // We can handle (tuple_extract x, 0) if x is an overflow-checking integer
  // operation.
  if (auto *TE = dyn_cast<TupleExtractInst>(inst)) {
    auto *op = dyn_cast<SILInstruction>((SILNode*)TE->getOperand());
    if (!op || TE->getFieldNo() != 0 || !isOverflowCheckingIntegerOp(op))
      return ScalarPromoteClass::NeverPromote;

    // We can only handle this tuple_extract if the underlying instruction can
    // be handled.  This can depend on dtype support, whether the overflow
    // flag is used, etc.
    return shouldPromoteToTensorOp(op);
  }

  // Check to see if we know how to promote this to a tensor operation.  If not,
  // we reject it.
  if (getPartitionedScalarOpName(inst).empty())
    return ScalarPromoteClass::NeverPromote;

  // TODO: We should do a bit more cost model analysis on this.  It doesn't make
  // sense to pull over a partial chain of computation when some root can't be
  // done there.  It is just shifting compute around.

  // If this is an integer or floating point literal, then it is cheaper to put
  // this in graph than it is to pass it as an argument (because then TensorFlow
  // can do constant propagation etc).  Don't attempt to hoist it ahead of the
  // start point.
  //
  // Cases that are handled here should be handled by
  // sinkValueIntoRegionForPromotion.
  if (isa<LiteralInst>(inst))
    return ScalarPromoteClass::ShouldPromote;

  // Otherwise, we can promote this if desired.
  return ScalarPromoteClass::CanPromote;
}

void TFFunctionPartition::markInstruction(SILInstruction &inst, Marking mark) {
  // Insert the specified instruction into the marked set.  If it is already
  // there then we have nothing more to do.
  if (!markedInstructions.insert({&inst, mark}).second)
    return;

  // If we're moving a computation to the accelerator, we can remove any
  // debug_value and retain/release instructions using this one.
  if (mark == Marking::Move) {
    for (auto result : inst.getResults())
      for (auto *operand : result->getUses()) {
        auto user = operand->getUser();
        if (isUserIgnoredByPartitioning(user))
          markInstruction(*user, Marking::Delete);
      }
  }

  // If we are simply deleting this instruction, then we're done.
  if (mark == Marking::Delete)
    return;

  // Make sure the instruction's block is marked as being copied over to the
  // tensor program.
  markBlock(inst.getParent());

  // If we have an uncond branch with basic block arguments, don't add operands
  // as used values.  Instead, we'll use more careful conditional liveness
  // based on whether the BB args in the successor are live.
  if (isa<BranchInst>(&inst))
    return;

  // If we are moving the instruction over, then we know it is a tensor op, and
  // it get special attention.  Otherwise, we just recursively mark the
  // operands in question.
  if (mark != Marking::Move) {
    auto operandRange = inst.getAllOperands();

    // Overflow-checking integer ops have a "should check" bit as their last
    // parameter, we don't remap it, so don't mark it.
    if (isOverflowCheckingIntegerOp(&inst)) {
      assert(operandRange.back().get()->getType().is<BuiltinIntegerType>());
      operandRange = operandRange.drop_back();
    }

    // Scan the operands to make sure they are available: either by moving the
    // computation over to the accelerator, by copying the value over, or by
    // passing as an argument to the tensor computation.
    for (auto &op : operandRange)
      markValue(op.get(), &inst);
    return;
  }

  // Okay, we know that the instruction is a tensor op.  Decode its argument
  // list so we know how to handle the operands.
  SILTensorOpInfo tfopInfo = SILTensorOpInfo::decode(&inst).getValue();
  unsigned nextOperand = 0;
  for (auto operandInfo : tfopInfo.operandDescriptors) {
    switch (operandInfo) {
    case OpDescriptor::Tensor:
      // Tensor operands are recursively marked.
      markValue(inst.getOperand(nextOperand++), &inst);
      break;
    case OpDescriptor::Scalar:
      // Scalar operands are recursively marked.
      markValue(tfopInfo.getScalarOperand(nextOperand++), &inst);
      break;
    case OpDescriptor::AddDType:
      // No operand to mark.
      break;
    case OpDescriptor::Constant:
      // No need to mark these operands.
      ++nextOperand;
      break;
    }
  }
}

void TFFunctionPartition::markArgument(SILArgument *arg, SILInstruction *user) {
  // If we've already marked this argument, there is nothing more to do.
  if (markedBBArguments.count(arg))
    return;

  // Make sure the argument's block is marked as being copied.
  markBlock(arg->getParent());

  // If this BB argument is outside the region dominated by the start point,
  // then we pass its value in as an argument to the tensor function.
  if (!DI.properlyDominates(tensorStartPoint->getParent(), arg->getParent())) {
    markedBBArguments.insert({arg, Marking::Argument});
    tensorFnArguments.push_back(SILValue(arg));
    diagnoseCopyInIfNotSend(arg, user);
    return;
  }

  // Otherwise, if this is a value of TensorHandle type, then we move it to the
  // accelerator.  If it is also used on the host, it will be copied back.
  if (isTensorHandle(arg->getType().getSwiftRValueType())) {
    // We cannot move over arguments, but they should never be in the dominated
    // region anyway.
    assert(!isa<SILFunctionArgument>(arg) &&
           "Cannot move function parameters!");
    markedBBArguments.insert({arg, Marking::Move});
  } else {
    markedBBArguments.insert({arg, Marking::Copy});
  }

  // Otherwise, we mark the branches that contribute values to it, then mark
  // their formal BB argument values that correspond to this argument.
  bool hasUncondBr = false;
  for (auto *pred : arg->getParent()->getPredecessorBlocks()) {
    auto predTerm = pred->getTerminator();
    hasUncondBr |= isa<BranchInst>(predTerm);
    markInstruction(*predTerm, Marking::Copy);
  }

  // We handle conditional liveness of "phi node" like arguments which are set
  // by unconditional branches.  Arguments resulting from other terminators are
  // handled eagerly.
  if (hasUncondBr) {
    SmallVector<SILValue, 4> incomingValues;
    arg->getIncomingValues(incomingValues);
    for (auto v : incomingValues)
      markValue(v, user);
  }
}

/// The specified instruction is in the region dominated by the start point of
/// the tensor computation and needs to be copied into it.  Try to hoist above
/// the start point, since we prefer arguments to the tensor program rather than
/// send and receives.  This returns true if it successfully hoists the
/// computation or if the value is already above the start point.
static bool hoistValueAboveStartPoint(SILInstruction *inst,
                                      SILInstruction *tensorStartPoint,
                                      DominanceInfo &DI) {
  // If this instruction already dominates the start point, then we're good to
  // go.  Don't move anything.
  if (DI.properlyDominates(inst, tensorStartPoint))
    return true;

  // In general, we need to check to see if we have a chain of side-effect free
  // instructions whose ultimate inputs dominate the start point.
  //
  // For now though, our implementation is extremely simple: we just check for
  // a couple of simple instructions that have come up so far.
  //
  // We can extend this in the future when there is a need.
  if (isa<StructExtractInst>(inst) || isa<LiteralInst>(inst)) {
    // We can hoist one of these instructions if all of their operands are
    // hoistable.
    for (auto &op : inst->getAllOperands()) {
      if (auto *opInst = dyn_cast<SILInstruction>((SILNode*)op.get())) {
        if (!hoistValueAboveStartPoint(opInst, tensorStartPoint, DI))
          return false;
      } else if (!DI.properlyDominates(op.get(), tensorStartPoint))
        return false;
    }

    // If all of the operands are hoisted above the start point, then this
    // instruction can be too.
    inst->moveBefore(tensorStartPoint);
    return true;
  }

  // Otherwise, we can't handle this instruction.
  return false;
}

/// The specified instruction is known to dominate the start point for the
/// program, and is known to be promotable to a tensor op.   Try to sink it down
/// to be part of the tensor program and return true if successful.
///
/// NOTE: This trivially simple implementation currently can't fail, but when
/// we generalize it later it can, so we return a bool to indicate success.
bool TFFunctionPartition::
sinkValueIntoRegionForPromotion(SILInstruction *inst) {
  // If this is too complex, don't try to sink it.
  // Right now, we just handle floating point and integer literals.
  assert(isa<LiteralInst>(inst) &&
         "can only handle 'ShouldPromote' cases identified by"
         " shouldPromoteToTensorOp");

  // We can't generally sink this beyond other users, so we clone the
  // instruction instead.
  inst = inst->clone(tensorStartPoint);
  tensorStartPoint = inst;

  // Replace uses of the original instruction with the new one, if they are
  // within the tensor program.
  for (auto result : inst->getResults()) {
    for (auto it = result->use_begin(), e = result->use_end(); it != e; ) {
      auto *operand = *it++;
      auto user = operand->getUser();

      // If the start point dominates this use, replace it.
      if (DI.dominates(tensorStartPoint, user))
        operand->set(cast<SingleValueInstruction>(inst));
    }
  }

  return true;
}

/// Indicate that the specified value must be available on the accelerator.
/// This can be done by moving the computation over, or by inserting a data
/// transfer.
void TFFunctionPartition::markValue(SILValue value, SILInstruction *user) {
  // We can safely ignore SILUndef, since SILCloner will just make another
  // one for us.
  if (isa<SILUndef>(value))
    return;

  if (auto *arg = dyn_cast<SILArgument>(value))
    return markArgument(arg, user);

  auto *inst = cast<SILInstruction>((SILNode*)value);
  if (markedInstructions.count(inst))
    return;

  // If this is a reference to a tensor op that we haven't gotten to yet, just
  // ignore it.  The outer marking loop will find it and mark it.
  if (tensorOpsSet.count(inst))
    return;

  // Determine whether the instruction is lexically before the tensor program
  // start point, and whether it is something we can promote into the graph.
  bool isBeforeStartPoint =
    !DI.properlyDominates(tensorStartPoint, inst);
  ScalarPromoteClass promotionClass = shouldPromoteToTensorOp(inst);

  // If this is a scalar operation that we really want to promote to a tensor
  // operation, then try to do so.
  if (promotionClass == ScalarPromoteClass::ShouldPromote) {
    // If the value is defined before of the program region, is used by the
    // tensor program, and if it is better to have it in the tensor program than
    // for it to be an argument, sink it into the tensor program and mark it.
    // This is useful for constants.  Consider code like this:
    //
    //   x = <opaque value>
    //   y = tensor_literal [1,2,3]
    //   z = x+y
    //
    // In this case, the operation "+" will be the start point, but we'd like to
    // sink the constant '1' into the region (it will actually become the new
    // start point).
    if (isBeforeStartPoint &&
        sinkValueIntoRegionForPromotion(inst))
      isBeforeStartPoint = false;

    // If the instruction is in the tensor program region (either because it was
    // already or because we just moved it) then we can mark it to be copied in.
    if (!isBeforeStartPoint)
      return markInstruction(*inst, Marking::Copy);
  }

  // If the value is defined outside of the region dominated by the tensor
  // operations (or it can be hoisted above it), then it is passed in as an
  // argument to the tensor function.
  if (isBeforeStartPoint ||
      // If we can hoist it above the start point then it can be an argument.
      hoistValueAboveStartPoint(inst, tensorStartPoint, DI)) {
    markedInstructions.insert({inst, Marking::Argument});
    tensorFnArguments.push_back(value);
    diagnoseCopyInIfNotSend(value, user);
    return;
  }

  // If this is a scalar operation that can be promoted to a tensor op on the
  // accelerator, mark it as being copied over (this leads to its operands
  // being recursively copied as well).
  if (promotionClass == ScalarPromoteClass::CanPromote)
    return markInstruction(*inst, Marking::Copy);

  // Otherwise, insert a send from the host to the accelerator.
  valuesToSend.insert(value);
  diagnoseCopyInIfNotSend(value, user);

  // Instead of cloning over this instruction, we'll add a send after it and
  // insert a receive in the accelerator code.
  markedInstructions.insert({inst, Marking::Send});
  markBlock(inst->getParent());
}

/// Given a list of tensor operations, find the nearest common ancestor of those
/// operations in the [post-]dominator-tree of the CFG.  In addition to finding
/// the NCA, this also returns the list of ops that are in that block (if any).
static SILBasicBlock *
findNCAOfTensorOps(ArrayRef<SILInstruction*> tensorOps,
                   SmallPtrSet<SILInstruction*, 8> &ncaBBOps,
         std::function<SILBasicBlock*(SILBasicBlock*,SILBasicBlock*)> findNCA) {
  assert(!tensorOps.empty() && "expect at least one tensor op");

  auto ncaBlock = tensorOps[0]->getParent(); // Arbitrary starting point.

  for (auto inst : tensorOps) {
    // If this op is in the ncaBlock, just remember it.
    auto instBB = inst->getParent();
    if (instBB == ncaBlock) {
      ncaBBOps.insert(inst);
      continue;
    }

    // Otherwise, it is possible that the startBB already dominates instBB.  If
    // so, the NCA Will not change.
    auto NCA = findNCA(ncaBlock, instBB);
    if (NCA == ncaBlock)
      continue;

    // Otherwise, the instBB dominated startBB, or neither dominated the other
    // one (meaning NCA is some unrelated parent block).  In either case, it
    // becomes our new startBB.
    ncaBlock = NCA;
    ncaBBOps.clear();
    ncaBBOps.insert(inst);
  }

  return ncaBlock;
}

/// Scan the function looking for blocks with tensor operations in them.  As
/// we find them, mark them as "to-be-partitioned", which marks (transitive)
/// data and control dependencies.
bool TFFunctionPartition::markFunction() {
  // We walk the function in depth first order so that we only visit reachable
  // blocks and to slightly improve compile time performance of the 'marking'
  // operation.
  SmallVector<SILInstruction*, 32> tensorOps;
  for (auto *BB : llvm::depth_first(&fn)) {
    for (auto &inst : *BB) {
      if (SILTensorOpInfo::decode(&inst)) {
        tensorOps.push_back(&inst);
        tensorOpsSet.insert(&inst);
      }
    }
  }

  // If there is nothing to do, don't touch this function.
  if (tensorOps.empty())
    return false;

  // Compute the blocksReachingTensorCode set.
  tensorCodeBlocks.compute(tensorOps);

  // Next, compute the NCA of all of the core tensor operations as our "start
  // point".  This is where we will start the tensor computation, sending over
  // argument values defined outside the scope of the computation.
  SmallPtrSet<SILInstruction*, 8> bbOps;
  auto startBB = findNCAOfTensorOps(tensorOps, bbOps,
                                    [&] (SILBasicBlock *B1,
                                         SILBasicBlock *B2) -> SILBasicBlock* {
    return DI.findNearestCommonDominator(B1, B2);
  });

  // Compute the start point by doing a linear scan of the startBB to find the
  // first (of possibly many) tensor operations.
  tensorStartPoint = startBB->getTerminator();
  if (!bbOps.empty()) {
    for (auto &inst : *startBB) {
      if (bbOps.count(&inst)) {
        tensorStartPoint = &inst;
        break;
      }
    }
  }

  // Find the end point by doing the same check using post dominators.
  bbOps.clear();
  auto endBB = findNCAOfTensorOps(tensorOps, bbOps,
                                  [&] (SILBasicBlock *B1,
                                         SILBasicBlock *B2) -> SILBasicBlock* {
    return tensorCodeBlocks.findNearestCommonPostDominator(B1, B2);
  });
  assert(endBB && "Didn't find an end point for the tensor program");

  // Compute the end point by doing a backward scan.
  if (bbOps.empty()) {
    tensorEndPoint = &endBB->front();
  } else {
    for (auto &inst : llvm::reverse(*endBB)) {
      if (bbOps.count(&inst))
        break;
      tensorEndPoint = &inst;
    }
  }

  // Now that we know the region we're extracting from, mark all of the
  // operations as being moved over to the graph, and recursively mark their
  // operands as appropriate.
  for (auto inst : tensorOps) {
    markInstruction(*inst, Marking::Move);

    // If the tensor operation is used by anything after the end point of the
    // region, then this can be modeled either as a result value of the program
    // or as a value sent from the accelerator to the host.  We prefer to model
    // it as an return value, so collect that now.
    for (auto result : inst->getResults())
      for (auto *operand : result->getUses()) {
        auto user = operand->getUser();

        // If the user is to be deleted, then we can safely ignore it.
        auto it = markedInstructions.find(user);
        if (it != markedInstructions.end() && it->second == Marking::Delete)
          continue;

        // If the end point dominates the out-of-model use, then we can
        // represent it with the return value of the tensor program.  Otherwise
        // it will turn into a send of data back to the host.
        if (DI.dominates(tensorEndPoint, user)) {
          resultValues.push_back(result);
          break;
        }
      }
  }

  return true;
}




//===----------------------------------------------------------------------===//
//                              PartitionCloner
//===----------------------------------------------------------------------===//

namespace {
class PartitionCloner : public SILClonerWithScopes<PartitionCloner> {
  TFFunctionPartition &FP;

  /// This is a basic block on the newly created function which represents the
  /// exit node of the function.
  SILBasicBlock *exitBB;

  /// This is a counter we use to give each send/receive operation a unique ID.
  unsigned nextSendID = 0;

  /// This is the set of instructions that should be removed from the host code
  /// after the cloning operation is complete.
  SmallVector<SILInstruction*, 8> instructionsToRemove;
public:
  PartitionCloner(TFFunctionPartition &FP, SILFunction &NewFn)
    : SILClonerWithScopes(NewFn), FP(FP) {
  }

  void cloneFunction();
  void finalizeOriginal();

  void insertSend(SILInstruction &inst);
  void insertReceive(SILValue value);

  // Handle references to blocks from cloned code.
  SILBasicBlock *remapBasicBlock(SILBasicBlock *BB) {
    // If the block is included in the partition, directly reference it.
    auto bbIt = BBMap.find(BB);
    if (bbIt != BBMap.end())
      return bbIt->second;

    // Otherwise, it must be a jump to a block that wasn't included in the
    // partition.  Figure out which post-dominated block is included, and jump
    // to it instead.
    auto pdiBlock = BB;
    while (1) {
      // If the post-idom is in the partition, use it.  Otherwise keep scanning.
      pdiBlock = FP.tensorCodeBlocks.getPostIDom(pdiBlock);

      // If we found the exit node, use our exitBB.
      if (!pdiBlock)
        return BBMap[BB] = exitBB;

      auto bbIt = BBMap.find(pdiBlock);
      if (bbIt != BBMap.end())
        return BBMap[BB] = bbIt->second;
    }
  }

  SILType remapType(SILType ty) {
    return convertToTensorHandleType(ty);
  }

  void visitOpInst(SingleValueInstruction *inst, SILTensorOpInfo &tfopInfo);
  void visitBuiltinInst(BuiltinInst *inst);
  void visitApplyInst(ApplyInst *inst);
  void visitLiteralInst(LiteralInst *inst);
  void visitIntegerLiteralInst(IntegerLiteralInst *inst) {
    visitLiteralInst(inst);
  }
  void visitFloatLiteralInst(FloatLiteralInst *inst) {
    visitLiteralInst(inst);
  }

  void visitTupleExtractInst(TupleExtractInst *inst);

  void visitBranchInst(BranchInst *inst);
  void visitCondBranchInst(CondBranchInst *inst);

private:
  // Check to see if the argument was marked in a way that indicates we should
  // copy it over to the tensor program.
  bool shouldCloneArgument(SILArgument *arg) const {
    auto it = FP.markedBBArguments.find(arg);
    if (it == FP.markedBBArguments.end()) return false;

    switch (it->second) {
    case Marking::Copy:
    case Marking::Move:
      return true;
    case Marking::Send:
    case Marking::Argument:
    case Marking::Delete:
      return false;
    }
  }

  void initBlock(SILBasicBlock *BB);
  void cloneBlock(SILBasicBlock *BB);
};
} // end anonymous namespace


/// We create each block in an initial pass over the function, before cloning
/// over the instructions.  This allows us to know that there is always a block
/// in our block mapping as we start cloning over branch instructions.
void PartitionCloner::initBlock(SILBasicBlock *BB) {
  auto newBB = Builder.getFunction().createBasicBlock();
  BBMap[BB] = newBB;

  // If the basic block has arguments, map over any marked ones.
  for (auto *arg : BB->getArguments()) {
    if (!shouldCloneArgument(arg))
      continue;

    // Create the argument and copy it into the ValueMap so future references
    // use it.
    ValueMap[arg] = newBB->createPHIArgument(remapType(arg->getType()),
                                             ValueOwnershipKind::Trivial,
                                             arg->getDecl());
  }

  // If this is the entry block for our computation, add the parameter BB
  // arguments.
  if (BB == FP.tensorStartPoint->getParent()) {
    for (auto arg : FP.tensorFnArguments) {
      auto argTy = convertToTensorHandleType(arg->getType());
      auto newArg = newBB->createFunctionArgument(argTy);
      ValueMap[arg] = SILValue(newArg);
    }
  }
}

// Provide special handling for unconditional branch instructions: we only
// clone over marked bb arguments, not all of them.
void PartitionCloner::visitBranchInst(BranchInst *inst) {
  SmallVector<SILValue, 4> operands;
  operands.reserve(inst->getNumArgs());
  auto destBB = inst->getDestBB();
  unsigned opNum = 0;
  for (auto &arg : inst->getAllOperands()) {
    if (shouldCloneArgument(destBB->getArgument(opNum++)))
      operands.push_back(remapValue(arg.get()));
  }

  getBuilder().setCurrentDebugScope(getOpScope(inst->getDebugScope()));
  auto br = getBuilder().createBranch(getOpLocation(inst->getLoc()),
                                      getOpBasicBlock(inst->getDestBB()),
                                      operands);
  doPostProcess(inst, br);
}

/// For conditional branches, we do exactly what the normal cloner does, except
/// that if we see a branch on a Tensor<Int1>, we unwrap it into an Int1.  We
/// know (by construction) that this only happens when the Tensor is a 0D value.
void PartitionCloner::visitCondBranchInst(CondBranchInst *inst) {
  auto TrueArgs = getOpValueArray<8>(inst->getTrueArgs());
  auto FalseArgs = getOpValueArray<8>(inst->getFalseArgs());
  auto &B = getBuilder();
  B.setCurrentDebugScope(getOpScope(inst->getDebugScope()));

  auto cond = getOpValue(inst->getCondition());

  if (auto eltTy = isTensorHandle(cond->getType().getSwiftRValueType())) {
    assert(eltTy->isBuiltinIntegerType(1) && "expected Tensor<i1>");

    auto name = B.getASTContext().getIdentifier("tf_tensor_to_i1");
    cond = B.createBuiltin(getOpLocation(inst->getLoc()), name,
                   SILType::getPrimitiveObjectType(eltTy->getCanonicalType()),
                           /*substitutionlist*/{}, cond);
  }

  doPostProcess(inst,
                B.createCondBranch(getOpLocation(inst->getLoc()),
                                   cond,
                                   getOpBasicBlock(inst->getTrueBB()), TrueArgs,
                                   getOpBasicBlock(inst->getFalseBB()),
                                   FalseArgs, inst->getTrueBBCount(),
                                   inst->getFalseBBCount()));
}


// Transform ops builtin instructions to the one we need in the tensor program.
void PartitionCloner::visitOpInst(SingleValueInstruction *inst,
                                  SILTensorOpInfo &tfopInfo) {
  // Handle special case "ops".
  if (tfopInfo.opName == "tfc.scalarToTensor") {
    assert(tfopInfo.operandDescriptorStr == "s" &&
           tfopInfo.resultDescriptorStr == "t" &&
           "invalid tfc.scalarToTensor!");

    // We just lower the result as the input, since the scalar input will have
    // been promoted to a tensor already.
    ValueMap[inst] = remapValue(tfopInfo.getScalarOperand(0));
    return;
  }


  auto &B = getBuilder();
  SmallVector<SILValue, 4> args;
  auto loc = remapLocation(inst->getLoc());

  auto cloneSingleInst = [&](SILInstruction *inst) -> SILInstruction* {
    auto ourInst = inst->clone();
    ourInst->setDebugLocation(B.getSILDebugLocation(loc));
    B.getInsertionBB()->push_back(ourInst);
    return ourInst;
  };

  // TODO: Attributes should support arrays as well as simple literals.
  auto cloneAttrInst = [&](SILInstruction *inst) -> SILInstruction* {
    return cloneSingleInst(inst);
  };

  unsigned nextOperand = isa<ApplyInst>(inst);  // Skip callee.
  for (auto operandInfo : tfopInfo.operandDescriptors) {
    switch (operandInfo) {
    case OpDescriptor::Tensor:
      // Tensor operands just become operands.
      args.push_back(remapValue(inst->getOperand(nextOperand++)));
      break;
    case OpDescriptor::Constant: {
      auto cst = tfopInfo.getTensorConstantOperand(nextOperand++);
      args.push_back(cloneSingleInst(cst)->getResults()[0]);
      break;
    }
    case OpDescriptor::AddDType:
      // Nothing to do for this.
      break;
    case OpDescriptor::Scalar:
      llvm_unreachable("scalar operands should always be handled specially");
    }
  }

  for (auto attrName : tfopInfo.attributeNames) {
    (void)attrName;  // Attrname is encoded in builtinName.
    auto attr = tfopInfo.getAttrOperand(nextOperand++);
    args.push_back(cloneAttrInst(attr)->getResults()[0]);
  }

  assert(nextOperand == inst->getNumOperands() &&
         "Some operands not consumed in TFPartition");

  auto name = B.getASTContext().getIdentifier(tfopInfo.builtinName);
  auto result = B.createBuiltin(loc, name, inst->getType(),
                                /*substitutionlist*/{}, args);
  ValueMap[inst] = result;
}

// If we're copying over a builtin instruction, it is due to a scalar operation
// that corresponds to an LLVM IR instruction.
void PartitionCloner::visitBuiltinInst(BuiltinInst *inst) {
  // Check to see if this is an op.
  if (auto tfopInfo = SILTensorOpInfo::decode(inst))
    return visitOpInst(inst, tfopInfo.getValue());

  // Otherwise, this is some other builtin we're partitioning, like an LLVM IR
  // instruction.
  auto opName = getPartitionedScalarOpName(inst);
  assert(!opName.empty() && "Should correspond to an op");
  auto &B = getBuilder();
  auto name = B.getASTContext().getIdentifier(opName);

  // Overflow checking integer instructions get special treatment.
  bool isOverflowCheckingInst = isOverflowCheckingIntegerOp(inst);


  // These are all simple things, just remap the operands directly.
  auto operandRange = inst->getAllOperands();

  // Overflow-checking integer ops have a "should check" bit as their last
  // parameter, which we don't remap.
  if (isOverflowCheckingInst)
    operandRange = operandRange.drop_back();

  SmallVector<SILValue, 4> operands;
  for (auto &op : operandRange)
    operands.push_back(remapValue(op.get()));

  // The type of the new builtin is usually the same as the input type, but
  // "remapped", which turns Float into TensorHandle<Float>.
  auto resultType = inst->getType();
  if (isOverflowCheckingInst) {
    // In the input program, cverflow checking instructions return something
    // like (Int64, i1).  During the marking process, we've determined that the
    // overflow bit is dead, so we only produce the normal result.
    resultType = resultType.getTupleElementType(0);
  }

  auto result =
    B.createBuiltin(remapLocation(inst->getLoc()), name,
                    remapType(resultType), /*substitutionlist*/{}, operands);
  ValueMap[inst] = result;
}

// If we're copying over an apply instruction, it is a function that we're
// promoting to a builtin in the graph.
void PartitionCloner::visitApplyInst(ApplyInst *inst) {
  // We know that this is an op, otherwise it won't have been marked.
  auto tfOpInfo = SILTensorOpInfo::decode(inst).getValue();
  return visitOpInst(inst, tfOpInfo);
}


/// We clone over simple literals like:
///
///    %X = integer_literal $Builtin.Int32, 0
/// into:
///    %Y = integer_literal $Builtin.Int32, 0
///    %X = builtin "tfop_Const"(%Y)
///
void PartitionCloner::visitLiteralInst(LiteralInst *inst) {
  auto loc = remapLocation(inst->getLoc());

  auto opName = getPartitionedScalarOpName(inst);
  assert(!opName.empty() && "Should correspond to an op");
  auto &B = getBuilder();

  auto name = B.getASTContext().getIdentifier(opName);
  auto ourCst = inst->clone();
  ourCst->setDebugLocation(B.getSILDebugLocation(loc));
  B.getInsertionBB()->push_back(ourCst);

  auto result =
    B.createBuiltin(loc, name, remapType(inst->getType()),
                    /*substitutionlist*/{}, { SILValue(ourCst) });
  ValueMap[inst] = result;
}


/// We clone over tuple_extract(x, 0) into x's value.  The only time
/// tuple_extract instructions get marked is when this is safe.
void PartitionCloner::visitTupleExtractInst(TupleExtractInst *inst) {
  assert(inst->getFieldNo() == 0 && "Unexpected extract to remap");
  ValueMap[inst] = remapValue(inst->getOperand());
}

static SILValue createReceive(SILBuilder &B, SILLocation loc,
                              SILType valueTy, unsigned idNumber) {
  auto &ctx = B.getASTContext();

  auto name = ctx.getIdentifier("tensorflowReceive_"+ llvm::utostr(idNumber));
  return // tensorflowReceive has type <T> () -> T
    B.createBuiltin(loc, name, valueTy,
                    Substitution(valueTy.getSwiftRValueType(), {}), {});
}

static void createSend(SILBuilder &B, SILLocation loc,
                       SILValue value, unsigned idNumber) {
  auto &ctx = B.getASTContext();
  auto voidTy = B.getModule().Types.getEmptyTupleType();
  auto name = ctx.getIdentifier("tensorflowSend_"+ llvm::utostr(idNumber));

  // tensorflowSend has type <T> (T) -> ()
  B.createBuiltin(loc, name, voidTy,
                  Substitution(value->getType().getSwiftRValueType(), {}),
                  {value});
}

/// Insert a send of values from the specified instruction result(s) to the
/// accelerator, and insert receives in it.
void PartitionCloner::insertSend(SILInstruction &inst) {
  assert(!isa<TermInst>(inst) && "Cannot insert after a terminator");

  SILBuilder BH(++SILBasicBlock::iterator(&inst)); // Builder for host.
  auto BA = getBuilder();        // Builder for accelerator.

  auto loc = getUserSourceLocation(inst.getDebugLocation());

  for (auto result : inst.getResults()) {
    // Create the receive in the accelerator code.  Each send/receive pair gets
    // a unique ID to associate one with the other.
    this->ValueMap[result] =
      createReceive(BA, loc, remapType(result->getType()), nextSendID);

    // Create the send in the host code.
    createSend(BH, loc, result, nextSendID);
    nextSendID++;
  }
}

void PartitionCloner::insertReceive(SILValue value) {
  assert(isa<SILInstruction>((SILNode*)value) || isa<SILArgument>(value) &&
         "Don't know how to receive this value");

  SILBuilder BH(FP.fn);          // Builder for the host.
  auto BA = getBuilder();        // Builder for accelerator.

  if (auto *inst = dyn_cast<SILInstruction>((SILNode*)value)) {
    assert(!isa<TermInst>(inst) && "Cannot move a terminator");
    BH.setInsertionPoint(++SILBasicBlock::iterator(inst));

    auto otherInst = cast<SILInstruction>((SILNode*)ValueMap[value]);
    BA.setInsertionPoint(++SILBasicBlock::iterator(otherInst));

  } else {
    auto *arg = cast<SILArgument>(value);
    BH.setInsertionPoint(&arg->getParent()->front());

    auto otherBB = remapBasicBlock(arg->getParent());
    BA.setInsertionPoint(&otherBB->front());
  }

  // Diagnose implicit data transfers.
  for (auto *use : value->getUses()) {
    FP.diagnoseCopyOutIfNotReceive(value, use->getUser());
  }

  auto loc = value.getLoc();
  if (auto *inst = value->getDefiningInstruction())
    loc = getUserSourceLocation(inst->getDebugLocation());

  // Create the send in the accelerator code.  Each send/receive pair gets
  // a unique ID to associate one with the other.
  createSend(BA, loc, remapValue(value), nextSendID);

  // Create the receive in the host code.
  auto newVal = createReceive(BH, loc, value->getType(), nextSendID);
  value->replaceAllUsesWith(newVal);
  nextSendID++;
}


/// Move and clone code over from the input block into this block, inserting
/// transfers between the host and destination code as necessary.
void PartitionCloner::cloneBlock(SILBasicBlock *BB) {
  if (!FP.markedBlocks.count(BB))
    return;  // Ignore blocks that aren't in accelerator code.
  auto newBB = BBMap[BB];

  Builder.setInsertionPoint(newBB);
  for (auto &inst : *BB) {
    // If the specified instruction is used by the accelerator program somehow,
    // we have to copy the instruction over, copy it, or send the result.
    auto it = FP.markedInstructions.find(&inst);
    if (it == FP.markedInstructions.end()) continue;

    switch (it->second) {
    case Marking::Move:
      instructionsToRemove.push_back(&inst);
      LLVM_FALLTHROUGH;
    case Marking::Copy:
      visit(&inst);
      break;
    case Marking::Send:
      insertSend(inst);
      break;
    case Marking::Argument:
      // Already handled.
      break;
    case Marking::Delete:
      instructionsToRemove.push_back(&inst);
      break;
    }
  }

  // If the terminator of this block wasn't live then it was either an
  // unconditional branch (which never matter for control dependence analysis)
  // or some kind of conditional branch that wasn't important because control
  // eventually flowed to a post dominator that didn't care about the direction
  // of this branch.  In either case, we can provide a terminator by introducing
  // an unconditional branch to the post dominator of the block.
  if (newBB->empty() || !isa<TermInst>(newBB->back())) {
    auto PDI = FP.tensorCodeBlocks.getPostIDom(BB);
    auto destBB = PDI ? remapBasicBlock(PDI) : exitBB;
    Builder.createBranch(BB->getTerminator()->getLoc(), destBB);
  }
}

void PartitionCloner::cloneFunction() {
  // Go through and create all the blocks before we start cloning the
  // instructions over.  This allows us to remap instructions when we clone
  // them over.
  initBlock(FP.tensorStartPoint->getParent());  // First block first.

  for (auto &BB : FP.fn) {
    // If the BB is unmarked, we don't need it for the accelerator.
    if (!FP.markedBlocks.count(&BB) ||
        &BB == FP.tensorStartPoint->getParent())
      continue;

    initBlock(&BB);
  }

  // Create a block for the exit node in case we need it.
  exitBB = Builder.getFunction().createBasicBlock();

  // Now that all the basic blocks and BBArguments are created, we can walk the
  // function in depth first order copying the code over.  Because we're working
  // in depth first order and have BB Arguments resolved, we're guaranteed to
  // see all definitions before uses.
  for (auto *BB : llvm::depth_first(&FP.fn)) {
    cloneBlock(BB);
  }

  // We can end up with to-be-deleted instructions (like debug_value's) outside
  // of the marked region.  These won't be seen in the cloneBlock walk we just
  // did, but we do want to remove them.  Check to see if we have any of these,
  // and arrange for them to be removed.
  for (auto ip : FP.markedInstructions) {
    if (ip.second == Marking::Delete &&
        !FP.markedBlocks.count(ip.first->getParent()))
      instructionsToRemove.push_back(ip.first);
  }

  // Okay at this point we're done except for setting up the exitBB.  Check to
  // see if it is unused.  If so, we nuke it, otherwise we add a return.
  if (exitBB->pred_empty()) {
    exitBB->eraseFromParent();
  } else {
    Builder.setInsertionPoint(exitBB);

    // Create a return of N values, producing a tuple if necessary.
    SILValue result;
    if (FP.resultValues.size() == 1)
      result = remapValue(FP.resultValues[0]);
    else {
      SmallVector<SILValue, 4> results;
      for (auto r : FP.resultValues)
        results.push_back(remapValue(r));

      result = Builder.createTuple(FP.fn.getLocation(), results);
    }

    Builder.createReturn(FP.fn.getLocation(), result);
  }
}


/// Now that all of the interesting instructions are cloned over, we need to
/// clean up the input function by removing the instructions, and inserting
/// sends of results from the accelerator code back to the host code.
///
void PartitionCloner::finalizeOriginal() {
  // Start by dropping interdependent references so we don't get confused by
  // uses that have moved over.
  for (auto i : instructionsToRemove) {
    // When removing a builtin tensor op instruction TO, for each tensor
    // argument TA of it, if TA is NOT generated by other tensor ops, emit a
    // strong_release to balance the already emitted strong_retain's for
    // TA. Recall the calling convention of TO is that TO consumes TA at +1.
    //
    // For example, before removing instruction
    // %r1 = builtin "__tfop_Add,tt:t"(%arg1, %arg2),
    // we first emit strong_release'es on %arg1 and %arg2.
    // If there is a subsequent instruction
    // %r2 = builtin "__tfop_Add,tt:t"(%r1, %arg3),
    // we only emit strong_release on %arg3, and not on %r1, since the
    // strong_retain on %r1 will be removed when all references of the first Add
    // instruction are dropped along with that instruction itself.
    if (FP.tensorOpsSet.count(i)) {
      SILBuilder BH(i);  // Builder for the host.
      for (auto& operand: i->getAllOperands()) {
        auto op = operand.get();
        if (!isTensorHandle(op->getType().getSwiftRValueType())) {
          continue;
        }
        // def could be NULL, say if the operand is produced by argument passed
        // into FP.fn.
        auto def = op->getDefiningInstruction();
        bool isDefinedByLocalTensorOp = def && FP.tensorOpsSet.count(def);
        if (!isDefinedByLocalTensorOp) {
          BH.emitStrongReleaseAndFold(i->getLoc(), op);
        }
      }
    }
    i->dropAllReferences();
  }

  // For BBArguments, we remove the uses from the branches first (which breaks
  // interdependent references) and delete the actual arguments later.
  for (auto arg : FP.markedBBArguments) {
    // If the argument is copied over, obviously don't zap it.
    if (arg.second != Marking::Move) {
      assert((arg.second == Marking::Copy || arg.second == Marking::Argument) &&
             "Only move/copy/argument supported for arguments right now");
      continue;
    }

    // Remove it from the block that it lives in.
    auto *bb = arg.first->getParent();
    auto argIndex = arg.first->getIndex();

    // Remove the formal values provided by any branches that jump to that
    // block.
    for (auto pi : bb->getPredecessorBlocks()) {
      auto *br = cast<BranchInst>(pi->getTerminator());
      SmallVector<SILValue, 8> operands;
      for (unsigned i = 0, e = br->getNumOperands(); i != e; ++i)
        if (i != argIndex)
          operands.push_back(br->getOperand(i));
      SILBuilder(br).createBranch(br->getLoc(), br->getDestBB(), operands);
      br->eraseFromParent();
    }
  }

  // Ok, now all interdependent references have been dropped.  If there are any
  // uses of values that we moved over to the accelerator, then we must insert
  // a receive from the accelerator of the computed value.  Regardless, we can
  // now delete the defining instruction/argument.
  for (auto argMarking : FP.markedBBArguments) {
    // If this is a copy, we don't need to do anything more.
    if (argMarking.second != Marking::Move)
      continue;
    auto arg = argMarking.first;

    // If the argument has any non-debug-non-retain/release instructions using
    // it, then we need to insert a copy.
    SmallVector<SILInstruction*, 2> instToRemove;
    bool needsCopy = false;
    for (auto operand : arg->getUses()) {
      auto user = operand->getUser();
      if (isUserIgnoredByPartitioning(user)) {
        instToRemove.push_back(user);
        continue;
      }
      needsCopy = true;
      break;
    }

    if (needsCopy)
      // If we leave around a copy we currently leave the retain/release ops
      // as well.
      // FIXME: Figure out a proper ownership story for the tensor values
      // flowing in and out of these runtime calls.
      insertReceive(arg);
    else {
      for (auto *inst : instToRemove)
        inst->eraseFromParent();
    }

    // Remove it from the block that it lives in.
    arg->getParent()->eraseArgument(arg->getIndex());
  }

  // Next, add sends back of values that are used by the host code, and remove
  // the original instructions.
  for (auto i : instructionsToRemove) {
    for (auto result : i->getResults())
      if (!result->use_empty())
        insertReceive(result);

    i->eraseFromParent();
  }

  // The copy-in/out markers should be removed now.  They are noops which serve
  // no purpose now that we have emitted diagnostics.
  for (auto ecm : FP.explicitCopyMarkers) {
    assert(isa<ApplyInst>(ecm) && ecm->getResults().size() == 1 &&
           ecm->getNumOperands() == 2 && "unknown copy in/out instruction");
    auto callee = ecm->getOperand(1);
    ecm->getResults()[0]->replaceAllUsesWith(ecm->getOperand(1));
    ecm->eraseFromParent();

    if (callee->use_empty())  // Remove the function_ref too.
      cast<SingleValueInstruction>(callee)->eraseFromParent();
  }
}

/// Wrap a value in a simple struct wrapper, these are common in the standard
/// library.
static SILValue wrapInStruct(SILValue v, NominalTypeDecl *decl, SILBuilder &B,
                             SILLocation loc) {
  auto type = decl->getDeclaredInterfaceType()->getCanonicalType();
  auto silType = SILType::getPrimitiveObjectType(type);
  return B.createStruct(loc, silType, v);
}

/// Create a value of some standard library integer type, as specified by
/// integerDecl.
static SILValue createSomeIntegerValue(intmax_t value, SILBuilder &B,
                                       SILLocation loc,
                                       NominalTypeDecl *integerDecl,
                                       IntegerLiteralInst **ILI = nullptr) {
  auto intFieldType = getSingleElementDeclFieldType(integerDecl);
  auto intFieldSILType = SILType::getPrimitiveObjectType(intFieldType);

  auto literal = B.createIntegerLiteral(loc, intFieldSILType, value);

  // If the caller wanted the integer_literal instruction, return it too.
  if (ILI) *ILI = literal;

  return wrapInStruct(literal, integerDecl, B, loc);
}

/// Create a value of 'Swift.Int' type holding the specified value.  It is a bit
/// trickier to create than some types because its contents are target platform
/// specific: Builtin.Int32 or Builtin.Int64.
static SILValue createIntValue(intmax_t value, SILBuilder &B, SILLocation loc,
                               IntegerLiteralInst **ILI = nullptr) {
  // Int should have one field, a Builtin.Int32/64.
  auto intDecl = B.getASTContext().getIntDecl();
  return createSomeIntegerValue(value, B, loc, intDecl, ILI);
}


/// Convert the specified scalar value (e.g. an i32) into a 0d CTensorHandle
/// value using the TensorFlow runtime utilities.
static SILValue convertScalarToHostTensorHandle(SILValue value, SILBuilder &B,
                                                SILLocation loc) {
  // We need to create a dtype value.  It is an imported C enum value, so it is
  // modeled as a struct that wraps an integer value (itself a struct).
  auto dtypeVal = convertSwiftTypeToTF(value->getType().getSwiftRValueType());
  assert(dtypeVal && "Can only convert TF compatible types to Tensors");

  // Get a reference to the CreateCTensorHandle function, which is defined like
  // this:
  // @_silgen_name("_swift_tfc_CreateCTensorHandle")
  // func _TFCCreateCTensorHandle<T>(_ value : T,
  //                                 _ dtype: TF_DataType) -> CTensorHandle
  auto createFn = B.getModule().findFunction("_swift_tfc_CreateCTensorHandle",
                                             SILLinkage::PublicExternal);
  auto *fnRef = B.createFunctionRef(loc, createFn);

  auto dtypeType = fnRef->getFunctionType()->getParameters()[1].getType();
  auto dtypeDecl = dtypeType->getAnyNominal();
  auto dtypeFieldType = getSingleElementDeclFieldType(dtypeDecl);

  // The internal type is something like UInt32.  Create it now.
  auto dtype = createSomeIntegerValue(dtypeVal, B, loc,
                                      dtypeFieldType->getAnyNominal());
  // Then wrap it in the dtype enum.
  dtype = wrapInStruct(dtype, dtypeDecl, B, loc);

  // This is a generic function over the value type, so we have to pass it in
  // by address on the stack.
  auto stackAlloc = B.createAllocStack(loc, value->getType());

  auto storeOwnership =
    B.getFunction().hasQualifiedOwnership() ? StoreOwnershipQualifier::Trivial :
                                          StoreOwnershipQualifier::Unqualified;
  B.createStore(loc, value, stackAlloc, storeOwnership);

  auto access = B.createBeginAccess(loc, stackAlloc, SILAccessKind::Read,
                                    SILAccessEnforcement::Static);

  Substitution sub(value->getType().getSwiftRValueType(), {});
  auto result = B.createApply(loc, fnRef, {sub}, { access, dtype },
                              /* isNonThrowing */false);
  // Finish our read access and free the stack memory.
  B.createEndAccess(loc, access, /*aborted*/false);
  B.createDeallocStack(loc, stackAlloc);

  return result;
}

/// Rewrite the host program, inserting a call to _TFCStartTensorComputation at
/// the start point of the tensor function, passing in the tensor program
/// itself, input tensor arguments etc.
auto TFFunctionPartition::
insertTensorComputationStartEndTerminate() -> PartitionedTensorProgram {
  auto &ctx = fn.getASTContext();
  auto loc = fn.getLocation();

  // We are going to create a call to this function to kick off the tensor
  // computation:
  //
  // The C type is TF_TensorHandle*
  // public typealias CTensorHandle = OpaquePointer
  //
  // @_silgen_name("_swift_tfc_StartTensorComputation")
  // public func _TFCStartTensorComputation(
  //   _ programByteAddress: UnsafeRawPointer,
  //   _ programByteCount: Int,
  //   _ entryFunctionName: UnsafePointer<Int8>,
  //   _ tensorArgumentAddress: UnsafePointer<CTensorHandle>,
  //   _ tensorArgumentCount: Int,
  //   _ resultCount: Int
  // ) -> TensorComputation {
  auto startComputationFn =
    fn.getModule().findFunction("_swift_tfc_StartTensorComputation",
                                SILLinkage::PublicExternal);

  // We are going to create a call to this function to syncronize with the
  // completed tensor computation and collect the results.
  //
  // @_silgen_name("_swift_tfc_FinishTensorComputation") public
  // func _TFCFinishTensonComputation(
  //    _ computation: TensorComputation,
  //    _ resultAddress: UnsafeMutablePointer<CTensorHandle>,
  //    _ tensorResultCount: Int) {...}
  auto finishComputationFn =
    fn.getModule().findFunction("_swift_tfc_FinishTensorComputation",
                                SILLinkage::PublicExternal);

  // We may also generate a call to this function to kill the tensor computation
  // if the host execution goes away:
  //
  // @_silgen_name("_swift_tfc_TerminateTensorComputation")
  // public func _TFCTerminateTensorComputation(
  //   _ computation: TensorComputation
  // ) {...}
  //
  auto terminateComputationFn =
    fn.getModule().findFunction("_swift_tfc_TerminateTensorComputation",
                                SILLinkage::PublicExternal);

  if (!startComputationFn || !finishComputationFn || !terminateComputationFn) {
    diagnose(ctx, fn.getLocation().getSourceLoc(),
             diag::tf_internal_error,
             "'_swift_tfc_' entrypoints not found in TensorFlow module");
    return { nullptr, nullptr, nullptr, nullptr, SILValue() };
  }

  // Create various types and SIL types that we'll be using below.
  auto cTensorHandleTy = ctx.getOpaquePointerDecl()->getDeclaredType();
  auto cTensorHandleSILTy =
    SILType::getPrimitiveObjectType(cTensorHandleTy->getCanonicalType());
  auto unsafePointerType =
    BoundGenericType::get(ctx.getUnsafePointerDecl(), /*parent*/Type(),
                          cTensorHandleTy);
  auto unsafePointerSILType =
    SILType::getPrimitiveObjectType(unsafePointerType->getCanonicalType());
  auto unsafeMutPointerType =
    BoundGenericType::get(ctx.getUnsafeMutablePointerDecl(), /*parent*/Type(),
                          cTensorHandleTy);
  auto unsafeMutPointerSILType =
    SILType::getPrimitiveObjectType(unsafeMutPointerType->getCanonicalType());
  auto int8PointerType =
    BoundGenericType::get(ctx.getUnsafePointerDecl(), /*parent*/Type(),
                          ctx.getInt8Decl()->getDeclaredType());
  auto int8PointerSILType =
    SILType::getPrimitiveObjectType(int8PointerType->getCanonicalType());

  // This assumes that the first member of TensorHandle is the CTensorHandle.
  auto tensorHandleDecl = ctx.getTensorHandleDecl();
  assert(getSingleElementDeclFieldType(tensorHandleDecl) &&
         "TensorHandle should have exactly one field");
  auto tensorHandleMember = *tensorHandleDecl->getStoredProperties().begin();

  // Ownership markers for CTensorHandle accesses.
  auto loadOwnership =
    fn.hasQualifiedOwnership() ? LoadOwnershipQualifier::Trivial :
                                 LoadOwnershipQualifier::Unqualified;
  auto storeOwnership =
    fn.hasQualifiedOwnership() ? StoreOwnershipQualifier::Trivial :
                                 StoreOwnershipQualifier::Unqualified;


  SILBuilder B(tensorStartPoint);

  // Create a string literal to hold the  serialized protobuf for the tensor
  // program.  We haven't actually created that yet, so we create a placeholder
  // and RAUW it later.
  auto programPlaceholder =
    B.createStringLiteral(loc, StringRef(), StringLiteralInst::Encoding::Bytes);
  auto program = wrapInStruct(programPlaceholder, ctx.getUnsafeRawPointerDecl(),
                              B, loc);
  auto entryFunctionNamePlaceholder =
    B.createStringLiteral(loc, StringRef(), StringLiteralInst::Encoding::UTF8);
  auto entryFunctionName = B.createStruct(loc, int8PointerSILType,
                                          { entryFunctionNamePlaceholder });

  // Pass a length of zero for now, it will be filled in later.
  IntegerLiteralInst *programLengthPlaceholder = nullptr;
  auto programLength = createIntValue(0, B, loc, &programLengthPlaceholder);

  // We pass the list of N tensor arguments as a pointer + length of
  // CTensorHandle values, i.e.:
  //   (..., _ inputs: UnsafePointer<CTensorHandle>, _ numInputs: Int)
  // to get this, we create an N-ary tuple on the stack and pass the address of
  // the first element.

  // Note that the allocation becomes a scalar value when it has one value.
  SmallVector<TupleTypeElt, 8> tupleEltTypes(tensorFnArguments.size(),
                                             TupleTypeElt(cTensorHandleTy));
  auto tupleType = TupleType::get(tupleEltTypes, ctx)->getCanonicalType();
  // %0 = alloc_stack $(CTensorHandle, CTensorHandle)
  auto stackAlloc =
    B.createAllocStack(loc, SILType::getPrimitiveObjectType(tupleType));

  // Emit a store into the tuple for each parameter, giving it a copy of the
  // OpaquePointer that is within the TensorHandle<T> value we have.
  for (size_t i = 0, numArgs = tensorFnArguments.size(); i != numArgs; ++i) {
    auto tensorValue = tensorFnArguments[i];

    // The argument is either a TensorHandle<T> or a scalar value that we've
    // closed over.  If it is a TensorHandle<T>, load the CTensorHandle out of
    // it.  If it is a scalar, then we need to box the scalar in a
    // CTensorHandle.
    if (isTensorHandle(tensorValue->getType().getSwiftRValueType())) {
      auto fieldAddress = B.createRefElementAddr(loc, tensorValue,
                                                 tensorHandleMember);
      tensorValue = B.createLoad(loc, fieldAddress, loadOwnership);
    } else {
      tensorValue = convertScalarToHostTensorHandle(tensorValue, B, loc);
    }
    SILValue eltAddr = stackAlloc;
    if (numArgs >= 2)
      eltAddr = B.createTupleElementAddr(loc, stackAlloc, i,
                                         cTensorHandleSILTy.getAddressType());
    B.createStore(loc, tensorValue, eltAddr, storeOwnership);
  }

  // Ok, now we have our array on the stack.  Start a read access, and get the
  // address of the first element as an UnsafePointer<CTensorHandle>.
  // %1 = begin_access [read] [static] %0 : $*(CTensorHandle, CTensorHandle)
  auto access = B.createBeginAccess(loc, stackAlloc, SILAccessKind::Read,
                                    SILAccessEnforcement::Static);

  // %2 = tuple_element_addr %1 : $*(CTensorHandle, CTensorHandle), 0
  SILValue firstPtr = access;
  if (tensorFnArguments.size() >= 2)
    firstPtr = B.createTupleElementAddr(loc, firstPtr, 0,
                                        cTensorHandleSILTy.getAddressType());
  // %3 = address_to_pointer %2 : $*CTensorHandle to $Builtin.RawPointer
  firstPtr = B.createAddressToPointer(loc, firstPtr,
                                      SILType::getRawPointerType(ctx));

  // %4 = struct $UnsafePointer<CTensorHandle>(%3 : $Builtin.RawPointer)
  firstPtr = B.createStruct(loc, unsafePointerSILType, firstPtr);

  auto numTensorArguments = createIntValue(tensorFnArguments.size(), B, loc);

  // TODO: When the runtime matures, we shouldn't have to pass in the # results.
  auto numTensorResults = createIntValue(resultValues.size(), B, loc);

  // The first two arguments are the program, the rest of the arguments are the
  // parameters passed in.
  SILValue startArgs[] = {
    program,            // programByteAddress: UnsafeRawPointer
    programLength,      // programByteCount: Int
    entryFunctionName,  // entryFunctionName: UnsafePointer<Int8>
    firstPtr,           // tensorArgumentAddress: UnsafePointer<CTensorHandle>
    numTensorArguments, // tensorArgumentCount: Int
    numTensorResults    // resultCount: Int
  };

  // Now that we have our argument list, create a call.
  auto startProgramFnRef = B.createFunctionRef(loc, startComputationFn);

  // Create the runtime call in the host program that kicks off the tensor
  // program, setting the argument values we provide as the tensor params.
  auto tensorComputation =
    B.createApply(loc, startProgramFnRef, /*no substitutions*/{}, startArgs,
                  /*isNonThrowing*/false);

  // Finish our read access and free the stack memory.
  // end_access %1 : $*(CTensorHandle, CTensorHandle)
  B.createEndAccess(loc, access, /*aborted*/false);
  // dealloc_stack %0 : $*(CTensorHandle, CTensorHandle)
  B.createDeallocStack(loc, stackAlloc);

  // Create the runtime call in the host program that rendezvous with the tensor
  // program and returns the results.

  // Start by creating an uninitialized buffer to receive the values into.
  B.setInsertionPoint(tensorEndPoint);

  // Note that the allocation becomes a scalar value when it has one value.
  tupleEltTypes = SmallVector<TupleTypeElt, 8>(resultValues.size(),
                                               TupleTypeElt(cTensorHandleTy));
  tupleType = TupleType::get(tupleEltTypes, ctx)->getCanonicalType();

  // %0 = alloc_stack $(CTensorHandle, CTensorHandle)
  stackAlloc =
    B.createAllocStack(loc, SILType::getPrimitiveObjectType(tupleType));

  // Ok, now we have our uninitialized array on the stack.  Start a write
  // access, and get the address of the first element as an
  // UnsafePointer<CTensorHandle>.
  // %1 = begin_access [write] [static] %0 : $*(CTensorHandle, CTensorHandle)
  access = B.createBeginAccess(loc, stackAlloc, SILAccessKind::Modify,
                               SILAccessEnforcement::Static);

  // %2 = tuple_element_addr %1 : $*(CTensorHandle, CTensorHandle), 0
  firstPtr = access;
  if (resultValues.size() >= 2)
    firstPtr = B.createTupleElementAddr(loc, firstPtr, 0,
                                        cTensorHandleSILTy.getAddressType());
  // %3 = address_to_pointer %2 : $*CTensorHandle to $Builtin.RawPointer
  firstPtr = B.createAddressToPointer(loc, firstPtr,
                                      SILType::getRawPointerType(ctx));

  // %4 = struct $UnsafeMutablePointer<CTensorHandle>(%3: $Builtin.RawPointer)
  firstPtr = B.createStruct(loc, unsafeMutPointerSILType, firstPtr);

  auto finishComputationFnRef = B.createFunctionRef(loc, finishComputationFn);

  // Create the builtin in the host program that kicks off the tensor program,
  // setting the argument values.
  B.createApply(loc, finishComputationFnRef, /*no substitutions*/{},
                /*args*/ {tensorComputation, firstPtr, numTensorResults},
                /*isNonThrowing*/false);

  // end_access %1 : $*(CTensorHandle, CTensorHandle)
  B.createEndAccess(loc, access, /*aborted*/false);

  // After the call, we have a buffer filled in with CTensorHandle values, load
  // them, taking ownership and RAUW'ing uses of the old value to the newly
  // loaded value.
  for (unsigned resultNumber = 0, e = resultValues.size(); resultNumber != e;
       ++resultNumber) {
    SILValue result = resultValues[resultNumber];
    SILValue eltAddress = stackAlloc;
    if (resultValues.size() > 1) {
      eltAddress =
        B.createTupleElementAddr(result.getLoc(),
                                 eltAddress, resultNumber,
                                 cTensorHandleSILTy.getAddressType());
    }

    // The load takes ownership from the buffer, leaving the buffer
    // uninitialized again.
    SILValue newValue = B.createLoad(result.getLoc(), eltAddress,
                                     loadOwnership);

    // Create a new TensorHandle<T> type to take ownership.
    auto newTH = B.createAllocRef(result.getLoc(), result->getType(),
                                  /*objc*/false, /*canAllocOnStack*/false,
                                  /*elementTypes*/{},
                                  /*elementCountOperands*/{});
    auto fieldAddress =
      B.createRefElementAddr(result.getLoc(), newTH, tensorHandleMember);

    B.createStore(result.getLoc(), newValue, fieldAddress,
                  storeOwnership);

    // Manually walk the use list in a custom way to avoid invalidating the
    // iterator as we potentially change it.
    for (auto UI = result->use_begin(), UE = result->use_end(); UI != UE; ) {
      auto *operand = *UI++;
      auto user = operand->getUser();

      // Users may be either inside (e.g. another tensor op, or a non-tensor
      // op that causes a copy back to the host) or outside the tensor
      // program.  If it is after the tensor op, we can replace the use with
      // the corresponding result value.  If inside, we'll nuke it later.
      if (DI.dominates(tensorEndPoint, user))
        operand->set(newTH);
    }
  }

  // Now that we are done with the buffer of results, get rid of it.  It is
  // uninitialized at this point, so it should not be destroyed.

  // dealloc_stack %0 : $*(CTensorHandle, CTensorHandle)
  B.createDeallocStack(loc, stackAlloc);



  // tensorComputation is the return value of _TFCStartTensorComputation.  By
  // construction, it is known to dominate all abort points and the finish point
  // point of the program.
  //
  // If the host program reaches any of the tensorKillBlocks, it should abort
  // execution of the tensor program.
  for (auto *killBB : tensorKillBlocks) {
    B.setInsertionPoint(&killBB->front());

    auto terminateComputationFnRef =
      B.createFunctionRef(loc, terminateComputationFn);

    // Create the builtin in the host program that kicks off the tensor program,
    // setting the argument values.
    B.createApply(loc, terminateComputationFnRef, /*no substitutions*/{},
                  /*args*/{ tensorComputation }, /*isNonThrowing*/false);
  }

  return {
    nullptr,  // New function hasn't been created yet.
    programPlaceholder,
    programLengthPlaceholder,
    entryFunctionNamePlaceholder,
    tensorComputation
  };
}


/// Run the TensorFlow partitioning pass.  This pass is a very close relative to
/// the standard "Aggressive Dead Code Elimination" (ADCE) optimization which is
/// implemented using post-dominance frontiers and control dependence
/// information, but instead of determining live code, we're determining
/// operations and a subset of the CFG that is profitable and interesting to
/// move to the accelerator.
///
/// This returns null if there is no tensor work to extract, or a function that
/// has been generated if there is.
///
auto TFFunctionPartition::partition() -> PartitionedTensorProgram {
  assert(!markedBlocks.empty() &&
         "Shouldn't run on functions with no tensor ops");

  // Insert the start/finish and any terminate runtime calls.
  auto result = insertTensorComputationStartEndTerminate();

  // If the TensorFlow module is malformed, bail out without breaking the code.
  if (!result.theTensorComputation)
    return result;

  // Calculate the parameter list for the new function.
  SmallVector<SILParameterInfo, 4> params;
  for (auto v : tensorFnArguments) {
    auto argTy = convertToTensorHandleType(v->getType());
    params.push_back(SILParameterInfo(argTy.getSwiftRValueType(),
                                      ParameterConvention::Direct_Unowned));
  }

  SmallVector<SILResultInfo, 4> results;
  for (auto r : resultValues)
    results.push_back(SILResultInfo(r->getType().getSwiftRValueType(),
                                    ResultConvention::Unowned));


  // Create the partitioned function, which never has arguments or result
  // values, since they get sent and received back and forth.
  auto newFnType =
    SILFunctionType::get(/*genericSig*/nullptr, SILFunctionType::ExtInfo(),
                         SILCoroutineKind::None,
                         ParameterConvention::Direct_Owned, params,
                         /*interfaceYields*/{},
                         results, /*interfaceErrorResult*/None,
                         fn.getModule().getASTContext());
  result.fn =
    fn.getModule().getOrCreateFunction(fn.getLocation(),
                                       fn.getName().str()+".tf_partition",
                                       SILLinkage::Private, newFnType,
                                       /*What's this*/IsBare, IsNotTransparent,
                                       IsNotSerialized);

  PartitionCloner PC(*this, *result.fn);

  // Fill in the cloned function body.
  PC.cloneFunction();

  // Clean up the source function, removing the tensor code.
  PC.finalizeOriginal();
  return result;
}


//===----------------------------------------------------------------------===//
//                              Top Level Driver
//===----------------------------------------------------------------------===//


// Our partitioning can leave around lots of unconditional branches between
// blocks that formerly had control edges.  Go through and merge those to make
// later passes simpler.
static void contractUncondBranches(SILFunction *fn) {
  // Iterate carefully to avoid invalidating iterators: we mutate the block list
  // while we walk it.
  for (auto bbi = fn->begin(), e = fn->end(); bbi != e; ) {
    auto *bb = &*bbi;
    ++bbi;  // Increment the iterator in case we do no transformation.

    if (auto succ = bb->getSingleSuccessorBlock()) {
      if (succ != bb && succ->getSinglePredecessorBlock()) {
        if (auto *BI = dyn_cast<BranchInst>(bb->getTerminator())) {
          // If there are any BB arguments in the destination, replace them with
          // the branch operands, since they must dominate the dest block.
          for (unsigned i = 0, e = BI->getArgs().size(); i != e; ++i) {
            assert(succ->getArgument(i) != BI->getArg(i) &&
                   "Cloned code regions are always reachable");
            succ->getArgument(i)->replaceAllUsesWith(BI->getArg(i));
          }

          // Zap BI and move all of the instructions from DestBB into this one.
          BI->eraseFromParent();
          bb->spliceAtEnd(succ);
          succ->eraseFromParent();

          // Revisit this node: we have new successor(s) and may need to
          // contract them as well.  Also, bbi may be invalidated at this point.
          bbi = SILFunction::iterator(bb);
        }
      }
    }
  }
}

/// This struct provides a single utility which determines whether a type is or
/// contains a TensorHandle that will be exposed after deabstraction.  This is a
/// class instead of a simple function because we memoize state to avoid
/// rechecking types over and over again.
namespace {
class TypeContainsTensorHandleChecker {
  /// This map memoizes whether the specified type declaration is known to
  /// contain a TensorHandle or not, used to accelerate queries against types
  /// that are frequently referenced like Tensor.
  llvm::DenseMap<NominalTypeDecl*, bool> declContainsTensorHandle;
public:
  TypeContainsTensorHandleChecker() {}

  /// Return true if the specified type contains a TensorHandle that will be
  /// exposed after deabstraction.
  bool containsTensorHandle(Type ty);

private:
  bool structContainsTensorHandle(StructDecl *decl);
};
} // end anonymous namespace


/// Return true if the specified type contains a TensorHandle that will be
/// exposed after deabstraction.
bool TypeContainsTensorHandleChecker::containsTensorHandle(Type ty) {
  // If this type literally is TensorHandle, then yep, we contain it.  This is
  // the base case.
  if (isTensorHandle(ty))
    return true;

  // Deabstraction flattens tuples, so if a tuple contains any tensor handles,
  // then the tuple itself does.
  if (auto *tuple = ty->getAs<TupleType>()) {
    for (auto &elt : tuple->getElements())
      if (containsTensorHandle(elt.getType()))
        return true;
    return false;
  }

  // Deabstraction scalarizes structs.
  if (auto *st = ty->getAs<StructType>())
    return structContainsTensorHandle(st->getDecl());

  // Deabstractions binds specialized generic structs.  Check if either the
  // struct itself or one of the generic arguments contains a TensorHandle.
  if (auto *bgst = ty->getAs<BoundGenericStructType>()) {
    // Check the generic arguments.
    for (auto arg : bgst->getGenericArgs())
      if (containsTensorHandle(arg))
        return true;

    return structContainsTensorHandle(bgst->getDecl());
  }

  // Handle still-generic types that may contain a TensorHandle.
  if (auto *ugst = ty->getAs<UnboundGenericType>())
    if (auto *decl = dyn_cast<StructDecl>(ugst->getDecl()))
      return structContainsTensorHandle(decl);

  // Otherwise we have a class or some other type that is opaque to
  // deabstraction.
  return false;
}

/// Determine whether the given struct contains a TensorHandle, caching the
/// result.
bool TypeContainsTensorHandleChecker::
structContainsTensorHandle(StructDecl *decl) {
  auto it = declContainsTensorHandle.find(decl);
  if (it != declContainsTensorHandle.end())
    return it->second;

  bool hasTensorHandle = false;
  for (auto p : decl->getStoredProperties())
    if (containsTensorHandle(p->getType())) {
      hasTensorHandle = true;
      break;
    }

  return declContainsTensorHandle[decl] = hasTensorHandle;
}

/// Determine whether the specified function should be partitioned (assuming it
/// contains tensor operations) or not.  This is important to determine up-front
/// because we want people to build libraries and other abstractions out of
/// tensor operations, and those tensor operations are not necessarily valid in
/// their generic form: for example, an op may require a constant, but may be
/// used inside layers of helpers: processing those helpers before they are
/// inlined away, will lead to us rejecting the op as invalid.
///
static bool shouldTransformFunction(SILFunction *fn,
                                    TypeContainsTensorHandleChecker &tcthc) {
  // Ignore transparent functions.
  if (fn->isTransparent())
    return false;

  auto hasInlinableAttrs = [&](Decl *decl) -> bool {
    if (decl->getAttrs().hasAttribute<InlineableAttr>())
      return true;
    if (auto attr = decl->getAttrs().getAttribute<InlineAttr>())
      if (attr->getKind() == InlineKind::Always)
        return true;
    return false;
  };

  // Don't transform functions that are marked @_inlineable or inline(always)
  if (auto dc = fn->getDeclContext()) {
    if (auto fnDecl = dc->getInnermostDeclarationDeclContext()) {
      if (hasInlinableAttrs(fnDecl))
        return false;

      // If this is an accessor for a computed property, check the property for
      // @_inlineable as well.
      if (auto *fd = dyn_cast<FuncDecl>(fnDecl)) {
        if (fd->isAccessor())
          if (hasInlinableAttrs(fd->getAccessorStorageDecl()))
            return false;
      }
    }
  }

  // If the function is marked public, but it isn't marked inlinable, then it is
  // a public entrypoint that cannot be deabstracted through, so we must
  // transform it.
  //
  // TODO: It will probably be a common error to forget to add the inlinable
  // attributes, we should either infer the attribute or produce better QoI that
  // suggests adding it when an error occurs.
  if (fn->getLinkage() == SILLinkage::Public)
    return true;

  // Something is creating public thunks around 'shared' implementations, which
  // prevents the above check from working.  Check for public functions.
  // FIXME: This should go away when we get deabstraction.
  if (fn->getLinkage() == SILLinkage::Shared)
    if (auto *dc = fn->getDeclContext())
      if (auto *fd = dyn_cast<FuncDecl>(dc))
        if (fd->getFormalAccess() >= AccessLevel::Public)
          return true;

  // Otherwise, the function is either public and inlininable or it is internal
  // to the current module.  In both cases, we check to see if the function
  // takes TensorHandle values as arguments or results.  If so, then we know
  // that it will be inlined away by deabstraction, and we don't need to touch
  // it.
  auto fnType = fn->getLoweredFunctionType();
  for (auto &result : fnType->getResults())
    if (tcthc.containsTensorHandle(result.getType()))
      return false;

  for (auto &param : fnType->getParameters())
    if (tcthc.containsTensorHandle(param.getType()))
      return false;

  // If it contains no tensor inputs or results, then we are willing to
  // transform it!
  return true;
}


namespace {
class TFPartition : public SILFunctionTransform {
  bool isTest = false;
  TypeContainsTensorHandleChecker tcthc;
public:
  TFPartition(bool isTest) : isTest(isTest) {}

  /// The entry point to the transformation.
  void run() override {
    SILFunction *fn = getFunction();
    auto &ctx = fn->getASTContext();

    // If the TensorFlow module hasn't been imported by the program, don't do
    // anything.  This avoids impacting compile time for non-TensorFlow using
    // Swift programs by doing extraneous analysis.
    auto tfModule = ctx.getLoadedModule(ctx.getIdentifier("TensorFlow"));
    if (!tfModule)
      return;

    // If this function is a building block of larger tensor programs (e.g.
    // the ops defined in the TensorFlow module), then don't transform it in
    // isolation.
    if (!shouldTransformFunction(fn, tcthc))
      return;

    // If this function has no source location information, then it came from
    // a deserialized module.  Don't transform it.
    if (fn->getLocation().isNull())
      return;

    TFFunctionPartition partitioner(*fn, PM, *tfModule);
    if (!partitioner.markFunction())
      return; // No tensor ops found in the function.

    // Check to see if we cannot transform the function but should.  In this
    // case we emit a compiler error.  This is a limitation of the compiler that
    // will need to be resolved in the future (possibly through a model change),
    // it's not clear if we should allow partitioning to work on unspecialized
    // generics.
    if (fn->getLoweredFunctionType()->isPolymorphic()) {
      diagnose(ctx, fn->getLocation().getSourceLoc(),
               diag::tf_internal_error,
              "TensorFlow partitioning does not work on generic functions yet");
      return;
    }

    // Because we're in active development, it is common to do something wrong
    // in the TensorFlow module.  Detect and reject things here.
    if (fn->getModule().getSwiftModule() == tfModule) {
      diagnose(ctx, fn->getLocation().getSourceLoc(),
               diag::tf_internal_error,
               "nothing in the TensorFlow module should require partitioning, "
               "did you forget @_inlineable on '" + fn->getName().str() + "'?");
      return;
    }

    if (shouldDumpIntermediates()) {
      llvm::outs() << "---- INPUT FUNCTION " << fn->getName() <<" ----------\n";
      fn->print(llvm::outs());
      llvm::outs() << "---- END OF INPUT FUNCTION ----------\n";
    }

    // Actually do the partitioning transformation, splitting out a new SIL
    // function for the tensor program body.
    auto tensorProgram = partitioner.partition();

    // If the TensorFlow module is malformed, exit without breaking the SIL:
    // an error has already been emitted.
    if (!tensorProgram.fn)
      return;

#ifndef NDEBUG
    // Verify that the generated function is ok.
    tensorProgram.fn->verify();
#endif

    // Our partitioning can leave around lots of unconditional branches between
    // blocks that formerly had control edges.  Go through and merge those to make
    // later passes simpler.
    contractUncondBranches(tensorProgram.fn);

    if (isTest || shouldDumpIntermediates()) {
      llvm::outs() << "--- TFPartition Accelerator Result: "
                   << tensorProgram.fn->getName() << "\n";
      tensorProgram.fn->print(llvm::outs());
      llvm::outs() << "----\n";
    }

    // If this is called from sil-opt, we currently just print out the results
    // and quit.  This allows writing regression tests for the tf-partition
    // pass in isolation.  This is pretty unconventional for a SIL pass, but
    // this is an unconventional pass!
    if (isTest) {
      llvm::outs() << "--- TFPartition Host Result: " << fn->getName() << "\n";
      fn->print(llvm::outs());
      llvm::outs() << "---\n";
      llvm::outs().flush();

      // Finally, we're done.  Remove the partitioned function so it doesn't go
      // through the normal compiler flow.
      tensorProgram.fn->getModule().eraseFunction(tensorProgram.fn);
      return;
    }


    // Next translate it to a graph and emit it as a global symbol.
    auto bytes = lowerTFGraph(tensorProgram.fn);

    // Now that we know what the tensor program actually is, we can replace the
    // placeholder instructions for the data + length with the actual bits we
    // want to use.
    {
      SILBuilder B(tensorProgram.programPlaceholder);
      auto data = B.createStringLiteral(fn->getLocation(),
                                        StringRef(bytes.data(), bytes.size()),
                                        StringLiteralInst::Encoding::Bytes);
      auto len = B.createIntegerLiteral(fn->getLocation(),
                              tensorProgram.programLengthPlaceholder->getType(),
                                        bytes.size());
      auto name = B.createStringLiteral(fn->getLocation(),
                                        tensorProgram.fn->getName(),
                                        StringLiteralInst::Encoding::UTF8);
      tensorProgram.programPlaceholder->replaceAllUsesWith(data);
      tensorProgram.programPlaceholder->eraseFromParent();
      tensorProgram.programLengthPlaceholder->replaceAllUsesWith(len);
      tensorProgram.programLengthPlaceholder->eraseFromParent();
      tensorProgram.entryFunctionNamePlaceholder->replaceAllUsesWith(name);
      tensorProgram.entryFunctionNamePlaceholder->eraseFromParent();
    }

    if (shouldDumpIntermediates()) {
      llvm::outs() << "--- TFPartition Host Result: " << fn->getName() << "\n";
      fn->print(llvm::outs());
      llvm::outs() << "---\n";
      llvm::outs().flush();
    }

    // Finally, we're done.  Remove the partitioned function so it doesn't go
    // through the normal compiler flow.
    tensorProgram.fn->getModule().eraseFunction(tensorProgram.fn);
  }
};

} // end anonymous namespace

SILTransform *swift::createTFPartition() {
  return new TFPartition(/*isTest*/false);
}

/// Create a version of the tf-partition pass that is used by sil-opt for
/// testcases.  TF-Partition is not a normal pass, so we need an unconventional
/// approach here.
SILTransform *swift::createTFPartitionTest() {
  return new TFPartition(/*isTest*/true);
}
