//===-- RankedTensor.swift.gyb --------------------------------*- swift -*-===//
//
// This source file is part of the Swift.org open source project
//
// Copyright (c) 2014 - 2017 Apple Inc. and the Swift project authors
// Licensed under Apache License v2.0 with Runtime Library Exception
//
// See https://swift.org/LICENSE.txt for license information
// See https://swift.org/CONTRIBUTORS.txt for the list of Swift project authors
//
//===----------------------------------------------------------------------===//
//
// Dynamically shaped but statically ranked tensors.
//
// Ranked tensors (Tensor1D, Tensor2D, etc) use an array of integers to
// represent a shape with known rank. The number of dimensions is guaranteed to
// equal the rank of the tensor.
//
//===----------------------------------------------------------------------===//

infix operator âŠ— : MultiplicationPrecedence

%{
  ranks = [1, 2, 3, 4]

  def rankName(rank):
    if rank == 1:
      return 'one'
    elif rank == 2:
      return 'two'
    elif rank == 3:
      return 'three'
    elif rank == 4:
      return 'four'

  def _elementArrayLiteral(rank):
    return 'Scalar' if rank == 1 else 'Tensor{}D<Scalar>'.format(rank-1)
}%

% for rank in ranks:
%   elementRank = rank - 1
%   liftedRank = rank + 1
%   TensorXD = 'Tensor{}D'.format(rank)
%   ArrayXD = 'Array{}D'.format(rank)
%   ElementTensor = 'Tensor{}D'.format(elementRank)
%   LiftedTensor = 'Tensor{}D'.format(liftedRank)
%   elementArrayLiteral = _elementArrayLiteral(rank)

/// A ${rankName(rank)}-dimensional tensor.
@_fixed_layout
public struct ${TensorXD}<Scalar : AccelerableByTensorFlow> : TensorProtocol {
  public typealias BoolTensor = ${TensorXD}<Bool>

  /// The underlying `Tensor` of the `${TensorXD}`.
  @_versioned
  internal var base: Tensor<Scalar>

  @_inlineable
  public static var rank: Int32 {
    @inline(__always)
    get {
      return ${rank}
    }
  }

  @_inlineable
  public var rank: Int32 {
    @inline(__always)
    get {
      return ${rank}
    }
  }

  @_inlineable
  public var scalars: [Scalar] {
    @inline(__always)
    get {
      return base.scalars
    }
  }

  @_inlineable
  public var scalarCount: Int32 {
    @inline(__always)
    get {
      return base.scalarCount
    }
  }

  @_inlineable
  public var handle: TensorHandle<Scalar> {
    @inline(__always)
    get {
      return base.handle
    }
  }

  @_inlineable
  public var shape: TensorShape {
    @inline(__always)
    get {
      return base.shape
    }
  }

  /// Creates a `${TensorXD}` from a `Tensor`.
  /// - Note: This is an internal initializer and should not be exposed to
  ///   users.
  @_versioned
  @_inlineable @inline(__always)
  internal init(base: Tensor<Scalar>) {
    self.base = base
  }

  @_inlineable @inline(__always)
  public init(handle: TensorHandle<Scalar>) {
    self.init(base: Tensor(handle: handle))
  }

  /// Creates a `${TensorXD}` from a `Tensor`. Returns nil if the `Tensor` does
  /// not have rank ${rank}.
  @_inlineable @inline(__always)
  public init?(_ other: Tensor<Scalar>) {
    guard other.rank == ${rank} else { return nil }
    self.init(base: other)
  }

  /// Creates a `${TensorXD}` from a `Tensor`.
  /// - Precondition: The tensor must have rank ${rank}.
  @_inlineable @inline(__always)
  public init(identicallyRanked other: Tensor<Scalar>) {
    // NOTE: Precondition written using Assert op instead of Swift
    // `precondition` to prevent send/receive.
    // FIXME: Remove #if when Assert works with a GPU build.
#if false
    let rankTensor = Tensor<Int32>(${rank})
    let condition: Tensor<Bool> = other.rankTensor == rankTensor
    #tfop("Assert", condition, [rankTensor, other.rankTensor]) as Void
#endif
    self.init(base: other)
  }

  /// Creates a `${TensorXD}` with the specified shape and contiguous scalars in
  /// row-major order.
  /// - Precondition: The shape must have ${rank} dimensions.
  /// - Precondition: The number of scalars must equal the product of the
  ///   dimensions of the shape.
  @_inlineable @inline(__always)
  public init(shape: TensorShape, scalars: [Scalar]) {
    // NOTE: Precondition written using Assert op instead of Swift
    // `precondition` to prevent send/receive.
    // TODO(danielzheng): Uncomment Assert op once receive bug is fixed here.
    // For some reason uncommenting even `rankTensor` below generates a receive.
    // let rankTensor = Tensor<Int32>(${rank})
    // let shapeTensor = Tensor<Int32>(shape.dimensions)
    // let condition: Tensor<Bool> = shapeTensor.scalarCountTensor == rankTensor
    // #tfop("Assert", condition, [rankTensor, shapeTensor]) as Void
    self.init(base: Tensor(shape: shape, scalars: scalars))
  }

  /// Creates a `${TensorXD}` with the specified shape and contiguous scalars in
  /// row-major order.
  /// - Precondition: The shape must have ${rank} dimensions.
  /// - Precondition: The number of scalars must equal the product of the
  ///   dimensions of the shape.
  @_inlineable @inline(__always)
  public init(shape: TensorShape, scalars: UnsafeBufferPointer<Scalar>) {
    // TODO: Precondition `scalars.count == shape.contiguousSize`, preferably in
    // graph.
    self.init(base: Tensor(shape: shape, scalars: scalars))
  }

  /// Creates a `${TensorXD}` with the specified shape and contiguous scalars in
  /// row-major order.
  /// - Precondition: The shape must have ${rank} dimensions.
  /// - Precondition: The number of scalars must equal the product of the
  ///   dimensions of the shape.
  @_inlineable @inline(__always)
  public init<C : RandomAccessCollection>(shape: TensorShape, scalars: C)
    where C.Element == Scalar {
    // TODO: Precondition `shape.count == ${rank}`, preferably in graph.
    self.init(base: Tensor(shape: shape, scalars: scalars))
  }

  /// Creates a `${TensorXD}` with the specified shape and a single, repeated
  /// value.
  /// - Precondition: The shape must have ${rank} dimensions.
  /// - Parameters:
  ///   - shape: The dimensions of the `${TensorXD}`.
  ///   - repeatedValue: The scalar value to repeat.
  @_inlineable @inline(__always)
  public init(shape: TensorShape, repeating repeatedValue: Scalar) {
    // NOTE: Precondition written using Assert op instead of Swift
    // `precondition` to prevent send/receive.
    // FIXME: Remove #if when Assert works with a GPU build.
#if false
    let rankTensor = Tensor<Int32>(${rank})
    let shapeTensor = Tensor<Int32>(shape.dimensions)
    let condition: Tensor<Bool> = shapeTensor.scalarCountTensor == rankTensor
    #tfop("Assert", condition, [rankTensor, shapeTensor]) as Void
#endif
    self.init(base: Tensor(shape: shape, repeating: repeatedValue))
  }
}

extension ${TensorXD} : Equatable where Scalar : Equatable {
  @_inlineable @inline(__always)
  public static func == (lhs: ${TensorXD}, rhs: ${TensorXD}) -> Bool {
    return lhs.elementsEqual(rhs).all()
  }

  @_inlineable @inline(__always)
  public static func != (lhs: ${TensorXD}, rhs: ${TensorXD}) -> Bool {
    return lhs.elementsNotEqual(rhs).any()
  }
}

/// Array literal support.
extension ${TensorXD} : ExpressibleByArrayLiteral {
  /// Creates a `${TensorXD}` from with the specified elements.
  @_inlineable @inline(__always)
  public init(arrayLiteral elements: ${elementArrayLiteral}...) {
    self.init(elements)
  }
}

public extension ${TensorXD} where Scalar : Numeric {
  /// Perform an element-wise type conversion from another `${TensorXD}`.
  @_inlineable @inline(__always)
  init<OtherScalar : Numeric>(_ other: ${TensorXD}<OtherScalar>) {
    self.init(base: Tensor<Scalar>(other.base))
  }
}

/// Initializers
public extension ${TensorXD} {
% if rank == 1:
  /// Creates a `${TensorXD}` from with the specified elements.
  @_inlineable @inline(__always)
  init<C : RandomAccessCollection>(_ elements: C)
    where C.Element == ${elementArrayLiteral} {
    self.init(base: Tensor<Scalar>(elements))
  }
% else:
  /// Creates a `${TensorXD}` from with the specified elements.
  @_inlineable @inline(__always)
  init<C : RandomAccessCollection>(_ elements: C)
    where C.Element == ${elementArrayLiteral} {
    self.init(base: Tensor<Scalar>(Array(elements)))
  }
% end

  /// Creates a `${TensorXD}` from a `ShapedArray`. Returns nil if the
  /// `ShapedArray` does not have rank ${rank}.
  @_inlineable @inline(__always)
  init?(_ array: ShapedArray<Scalar>) {
    self.init(Tensor(array))
  }

  /// Creates a `${TensorXD}` from a `ShapedArray`.
  /// - Precondition: The `ShapedArray` must have rank ${rank}.
  @_inlineable @inline(__always)
  init(identicallyRanked other: ShapedArray<Scalar>) {
    self.init(identicallyRanked: Tensor(other))
  }

% if rank != 1:
  /// Creates a `${TensorXD}` from an `${ArrayXD}`.
  @_inlineable @inline(__always)
  init(_ array: ${ArrayXD}<Scalar>) {
    self.init(base: Tensor(array.base))
  }
% end

  /// Creates a `${TensorXD}` containing the given element literals.
  @_inlineable @inline(__always)
  init(_ elements: [${elementArrayLiteral}]) {
    self.init(base: Tensor<Scalar>(elements))
  }

  /// Creates a `${TensorXD}` containing the given element literals.
  @_inlineable @inline(__always)
  init(_ elements: ${elementArrayLiteral}...) {
    self.init(elements)
  }

  /// Creates a `${TensorXD}` by broadcasting the given scalar to a {rank}-D
  /// shape with all dimensions being 1.
  @_inlineable @inline(__always)
  init(broadcasting scalar: Scalar) {
    self.init(shape: ${[1] * rank}, scalars: [scalar])
  }
}

public extension AccelerableByTensorFlow {
  /// Converts to a `${TensorXD}` with all dimensions equal to 1.
  @_inlineable @inline(__always)
  func make${TensorXD}() -> ${TensorXD}<Self> {
    return ${TensorXD}(base: self.makeTensor(withRank: ${rank}))
  }
}

/// Memory transfer markers
/// - TODO: Remove when send/receive semantics gets revisited.
public extension ${TensorXD} {
  /// Indicate that this tensor is being moved to the accelerator.
  @_inlineable @inline(__always)
  func toDevice() -> ${TensorXD} {
    return ${TensorXD}(base: base.toDevice())
  }

  /// Indicate that this tensor is being moved to the host.
  @_inlineable @inline(__always)
  func toHost() -> ${TensorXD} {
    return ${TensorXD}(base: base.toHost())
  }
}

/// Factories
public extension ${TensorXD} where Scalar : Numeric {
  /// Creates a `${TensorXD}` with all scalars set to zero.
  ///
  /// - Parameter shape: The dimensions of the `${TensorXD}`.
  @_inlineable @inline(__always)
  init(zeros shape: TensorShape) {
    // NOTE: Precondition written using Assert op instead of Swift
    // `precondition` to prevent send/receive.
    // FIXME: Remove #if when Assert works with a GPU build.
#if false
    let rankTensor = Tensor<Int32>(${rank})
    let shapeTensor = Tensor<Int32>(shape.dimensions)
    let condition: Tensor<Bool> = shapeTensor.scalarCountTensor == rankTensor
    #tfop("Assert", condition, [rankTensor, shapeTensor]) as Void
#endif
    self.init(base: Tensor<Scalar>(zeros: shape))
  }

  /// Creates a `${TensorXD}` with all scalars set to one.
  ///
  /// - Parameter shape: The dimensions of the `${TensorXD}`.
  @_inlineable @inline(__always)
  init(ones shape: TensorShape) {
    // NOTE: Precondition written using Assert op instead of Swift
    // `precondition` to prevent send/receive.
    // FIXME: Remove #if when Assert works with a GPU build.
#if false
    let rankTensor = Tensor<Int32>(${rank})
    let shapeTensor = Tensor<Int32>(shape.dimensions)
    let condition: Tensor<Bool> = shapeTensor.scalarCountTensor == rankTensor
    #tfop("Assert", condition, [rankTensor, shapeTensor]) as Void
#endif
    self.init(base: Tensor<Scalar>(ones: shape))
  }
}

public extension ${TensorXD} where Scalar == Float {
  /// Creates a `${TensorXD}` with the specified shape, randomly sampling
  /// scalar values from a normal distribution.
  ///
  /// - Parameters:
  ///   - shape: The dimensions of the `${TensorXD}`.
  ///   - mean: The mean of the distribution.
  ///   - stddev: The standard deviation of the distribution.
  ///   - state: The pseudorandom state in which the random numbers are being
  ///     generated.
  ///
  @_inlineable @inline(__always)
  init(randomNormal shape: TensorShape, mean: Scalar = 0, stddev: Scalar = 1,
       state: RandomState? = nil) {
    precondition(shape.count == ${rank},
                 "The given shape doesn't match rank ${rank}")
    self.init(base: Tensor(randomNormal: shape, mean: mean, stddev: stddev,
                           state: state))
  }

  /// Creates a `{TensorXD}` with the specified shape, randomly sampling scalar
  /// values from a uniform distribution.
  ///
  /// - Parameters:
  ///   - shape: The dimensions of the `${TensorXD}`.
  ///   - min: The lower bound of the distribution.
  ///   - max: The upper bound of the distribution.
  ///   - state: The pseudorandom state in which the random numbers are being
  ///     generated.
  ///
  @_inlineable @inline(__always)
  init(randomUniform shape: TensorShape, state: RandomState? = nil) {
    precondition(shape.count == ${rank},
                 "The given shape doesn't match rank ${rank}")
    self.init(base: Tensor(randomUniform: shape, state: state))
  }
}

public extension ${TensorXD} where Scalar == Double {
  /// Creates a `${TensorXD}` with the specified shape, randomly sampling
  /// scalar values from a normal distribution.
  ///
  /// - Parameters:
  ///   - shape: The dimensions of the `${TensorXD}`.
  ///   - mean: The mean of the distribution.
  ///   - stddev: The standard deviation of the distribution.
  ///   - state: The pseudorandom state in which the random numbers are being
  ///     generated.
  ///
  @_inlineable @inline(__always)
  init(randomNormal shape: TensorShape, mean: Scalar = 0, stddev: Scalar = 1,
       state: RandomState? = nil) {
    precondition(shape.count == ${rank},
                 "The given shape doesn't match rank ${rank}")
    self.init(base: Tensor(randomNormal: shape, mean: mean, stddev: stddev,
                           state: state))
  }

  /// Creates a `${TensorXD}` with the specified shape, randomly sampling scalar
  /// values from a uniform distribution.
  ///
  /// - Parameters:
  ///   - shape: The dimensions of the `${TensorXD}`.
  ///   - min: The lower bound of the distribution.
  ///   - max: The upper bound of the distribution.
  ///   - state: The pseudorandom state in which the random numbers are being
  ///     generated.
  ///
  @_inlineable @inline(__always)
  init(randomUniform shape: TensorShape, state: RandomState? = nil) {
    precondition(shape.count == ${rank},
                 "The given shape doesn't match rank ${rank}")
    self.init(base: Tensor(randomUniform: shape, state: state))
  }
}

//===----------------------------------------------------------------------===//
// Automatic differentiation
//===----------------------------------------------------------------------===//

extension ${TensorXD} : Differentiable where Scalar : FloatingPoint {
  /// The currency type in the mathematical model of differentiation.
  public typealias DifferentiationCurrency = Scalar

  /// Creates an instance by numerically broadcasting the specified currency
  /// value to be structurally isomorphic to another instance.
  ///
  /// - Parameters:
  ///   - value: The differentiation currency value for initializing the
  ///     instance.
  ///   - other: The other structurally isomorphic instance.
  ///
  @_inlineable @inline(__always)
  public init(numericallyBroadcasting value: Scalar, to other: ${TensorXD}) {
    self.init(handle: #tfop("Fill", other.shapeTensor, value))
  }

  @_inlineable @inline(__always)
  public func combiningAsAdjoint(with other: ${TensorXD}) -> ${TensorXD} {
    return self + other
  }
}

//===----------------------------------------------------------------------===//
// Transforms
//===----------------------------------------------------------------------===//

public extension ${TensorXD} {
  /// Returns a transposed `${TensorXD}`, with dimensions permuted in reverse
  /// order.
  @_inlineable @inline(__always)
  func transposed() -> ${TensorXD} {
    let defaultPermutations = rankTensor - 1 - Tensor<Int32>(
      rangeFrom: 0, to: rank, stride: 1
    )
    return transposed(withPermutations: defaultPermutations)
  }

  /// Return a copy of the `${TensorXD}` collapsed into a `Tensor1D` in
  /// row-major order.
  @_inlineable @inline(__always)
  func flattened() -> Tensor1D<Scalar> {
    return Tensor1D(base: base.flattened())
  }
}

//===----------------------------------------------------------------------===//
// Indexing and slicing
//===----------------------------------------------------------------------===//

public extension ${TensorXD} {
% if rank == 1:
  @_inlineable
  /// Access the scalar specified by a given index.
  /// - Parameter index: Index of the scalar.
  subscript(index: Int32) -> Scalar {
    @inline(__always)
    get {
      return _TFGetScalarOrDie(base[index].handle)
    }
  }
% else:
  @_inlineable
  /// Access the element `${ElementTensor}` specified by an index in the leading
  /// dimension.
  /// - Parameter index: Index of the element `${ElementTensor}`.
  subscript(index: Int32) -> ${ElementTensor}<Scalar> {
    @inline(__always)
    get {
      return ${ElementTensor}<Scalar>(base: base[index])
    }
  }
% end

  /// Access the subtensor specified by a contiguous range of indices.
  /// - Parameter bounds: Contiguous range of indices.
  @_inlineable
  subscript(bounds: Range<Int32>) -> ${TensorXD} {
    @inline(__always)
    get {
      return ${TensorXD}(base: base[bounds])
    }
  }

  /// Extracts a slice from a `${TensorXD}`. This operation extracts a slice at
  /// the ranges specified by `dimensionalBounds`.
  ///
  /// - Parameter dimensionalBounds: Bounds at each dimension.
  ///
  @_inlineable @inline(__always)
  func slice(lowerBounds: [Int32], upperBounds: [Int32]) -> ${TensorXD} {
    /// TODO: Precondition `lowerBounds.count == upperBounds.count`, and
    /// `lowerBounds.count == rank`, preferably in graph.
    return ${TensorXD}(base: base.slice(lowerBounds: lowerBounds,
                                        upperBounds: upperBounds))
  }
}

//===----------------------------------------------------------------------===//
// Reduction
//===----------------------------------------------------------------------===//

% if rank > 1:
public extension ${TensorXD} where Scalar : Numeric {
  // NOTE: This overload is necessary, otherwise `sum()` would refer
  // to the variadic method `sum(squeezingAxis:)` with zero indices.
  @_inlineable @inline(__always)
  func sum() -> Scalar {
    let axes = Tensor<Int32>(rangeFrom: 0, to: rank, stride: 1)
    return _TFGetScalarOrDie(#tfop("Sum", self, axes))
  }

  // NOTE: This overload is necessary, otherwise `mean()` would refer
  // to the variadic method `mean(squeezingAxis:)` with zero indices.
  @_inlineable @inline(__always)
  func mean() -> Scalar {
    let axes = Tensor<Int32>(rangeFrom: 0, to: rank, stride: 1)
    return _TFGetScalarOrDie(#tfop("Mean", self, axes))
  }


  @_inlineable @inline(__always)
  func sum(squeezingAxis axis: Int32) -> ${ElementTensor}<Scalar> {
    return ${ElementTensor}<Scalar>(base: base.sum(squeezingAxes: axis))
  }

  @_inlineable @inline(__always)
  func mean(squeezingAxis axis: Int32) -> ${ElementTensor}<Scalar> {
    return ${ElementTensor}<Scalar>(base: base.mean(squeezingAxes: axis))
  }
}

public extension ${TensorXD} where Scalar : Numeric & Comparable {
  // NOTE: This overload is necessary, otherwise `min()` would refer
  // to the variadic method `min(squeezingAxis:)` with zero indices.
  @_inlineable @inline(__always)
  func min() -> Scalar {
    let axes = Tensor<Int32>(rangeFrom: 0, to: rank, stride: 1)
    return _TFGetScalarOrDie(#tfop("Min", self, axes))
  }

  // NOTE: This overload is necessary, otherwise `max()` would refer
  // to the variadic method `max(squeezingAxis:)` with zero indices.
  @_inlineable @inline(__always)
  func max() -> Scalar {
    let axes = Tensor<Int32>(rangeFrom: 0, to: rank, stride: 1)
    return _TFGetScalarOrDie(#tfop("Max", self, axes))
  }

  @_inlineable @inline(__always)
  func min(squeezingAxis axis: Int32) -> ${ElementTensor}<Scalar> {
    return ${ElementTensor}<Scalar>(base: base.min(squeezingAxes: axis))
  }

  @_inlineable @inline(__always)
  func max(squeezingAxis axis: Int32) -> ${ElementTensor}<Scalar> {
    return ${ElementTensor}<Scalar>(base: base.max(squeezingAxes: axis))
  }

  @_inlineable @inline(__always)
  func argmax(squeezingAxis axis: Int32) -> ${ElementTensor}<Int32> {
    return ${ElementTensor}<Int32>(base: base.argmax(squeezingAxis: axis))
  }

  @_inlineable @inline(__always)
  func argmin(squeezingAxis axis: Int32) -> ${ElementTensor}<Int32> {
    return ${ElementTensor}<Int32>(base: base.argmin(squeezingAxis: axis))
  }
}

public extension ${TensorXD} where Scalar == Bool {
  // NOTE: This overload is necessary, otherwise `all()` would refer
  // to the variadic method `all(squeezingAxis:)` with zero indices.
  @_inlineable @inline(__always)
  func all() -> Bool {
    let axes = Tensor<Int32>(rangeFrom: 0, to: rank, stride: 1)
    return _TFGetScalarOrDie(#tfop("All", self, axes))
  }

  // NOTE: This overload is necessary, otherwise `any()` would refer
  // to the variadic method `any(squeezingAxis:)` with zero indices.
  @_inlineable @inline(__always)
  func any() -> Bool {
    let axes = Tensor<Int32>(rangeFrom: 0, to: rank, stride: 1)
    return _TFGetScalarOrDie(#tfop("Any", self, axes))
  }

  @_inlineable @inline(__always)
  func all(squeezingAxis axis: Int32) -> ${ElementTensor}<Scalar> {
    return ${ElementTensor}<Scalar>(base: base.all(squeezingAxes: axis))
  }

  @_inlineable @inline(__always)
  func any(squeezingAxis axis: Int32) -> ${ElementTensor}<Scalar> {
    return ${ElementTensor}<Scalar>(base: base.any(squeezingAxes: axis))
  }
}
% end

public extension ${TensorXD} {
% if rank < ranks[-1]:
  /// Returns a rank-lifted `{LiftedTensor}` with a leading dimension of 1.
  @_inlineable @inline(__always)
  func rankLifted() -> ${LiftedTensor}<Scalar> {
    return ${LiftedTensor}<Scalar>(base: base.rankLifted())
  }

  /// Returns a shape-expanded `{LiftedTensor}`, inserting a dimension of 1 at
  /// the specified shape index.
  @_inlineable @inline(__always)
  func expandingShape(at shapeIndex: Int32) -> ${LiftedTensor}<Scalar> {
    return ${LiftedTensor}<Scalar>(base: base.expandingShape(at: shapeIndex))
  }
% end
}

//===----------------------------------------------------------------------===//
// Description and visualization
//===----------------------------------------------------------------------===//

/// String conversion.
extension ${TensorXD} : CustomStringConvertible {
  @_inlineable
  public var description: String {
    return base.description
  }
}

/// Xcode Playground display conversion.
extension ${TensorXD} : CustomPlaygroundDisplayConvertible {
  public var playgroundDescription: Any {
    return description
  }
}

/// Mirror representation, used by debugger/REPL.
extension ${TensorXD} : CustomReflectable {
  public var customMirror: Mirror {
    return Mirror(self, children: [], displayStyle: .struct)
  }
}

//===----------------------------------------------------------------------===//
// Array conversion
//===----------------------------------------------------------------------===//

public extension ${TensorXD} {
  @_inlineable
  var array: ${ArrayXD}<Scalar> {
    @inline(__always)
    get {
      // This is considered to be a well known way to produce a copy to the
      // host, so an "implicit copy to host" warning should not be produced.
% if rank == 1:
      return base.scalars
% else:
      return ${ArrayXD}(base: base.array)
% end
    }
  }
}
% end

//===----------------------------------------------------------------------===//
// Tensor conversion
//===----------------------------------------------------------------------===//

/// Value-preserving conversion initializer
public extension Tensor {
% for rank in ranks:
%   TensorXD = 'Tensor{}D'.format(rank)
  @_inlineable @inline(__always)
  init(_ tensor: ${TensorXD}<Scalar>) {
    self = tensor.base
  }
% end
}
