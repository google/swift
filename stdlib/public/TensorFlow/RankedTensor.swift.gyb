//===-- RankedTensor.swift.gyb --------------------------------*- swift -*-===//
//
// This source file is part of the Swift.org open source project
//
// Copyright (c) 2014 - 2017 Apple Inc. and the Swift project authors
// Licensed under Apache License v2.0 with Runtime Library Exception
//
// See https://swift.org/LICENSE.txt for license information
// See https://swift.org/CONTRIBUTORS.txt for the list of Swift project authors
//
//===----------------------------------------------------------------------===//
//
// Dynamically shaped but statically ranked tensors.
//
// Ranked tensors (Tensor1D, Tensor2D, etc) use an array of integers to
// represent a shape with known rank. The number of dimensions is guaranteed to
// equal the rank of the tensor.
//
//===----------------------------------------------------------------------===//

infix operator âŠ— : MultiplicationPrecedence

%{
  ranks = [1, 2, 3, 4]

  def rankName(rank):
    if rank == 1:
      return 'one'
    elif rank == 2:
      return 'two'
    elif rank == 3:
      return 'three'
    elif rank == 4:
      return 'four'

  def _elementArrayLiteral(rank):
    return 'Scalar' if rank == 1 else 'Tensor{}D<Scalar>'.format(rank-1)
}%

% for rank in ranks:
%   elementRank = rank - 1
%   liftedRank = rank + 1
%   TensorXD = 'Tensor{}D'.format(rank)
%   ArrayXD = 'Array{}D'.format(rank)
%   ElementTensor = 'Tensor{}D'.format(elementRank)
%   LiftedTensor = 'Tensor{}D'.format(liftedRank)
%   elementArrayLiteral = _elementArrayLiteral(rank)

/// A ${rankName(rank)}-dimensional tensor.
@_fixed_layout
public struct ${TensorXD}<Scalar : AccelerableByTensorFlow> {
  /// The underlying Tensor of the ${TensorXD}.
  @_versioned
  internal var base: Tensor<Scalar>

  @_inlineable
  public static var rank: Int32 {
    @inline(__always)
    get {
      return ${rank}
    }
  }

  @_inlineable
  public var rank: Int32 {
    @inline(__always)
    get {
      return ${rank}
    }
  }

  @_inlineable
  public var scalars: [Scalar] {
    @inline(__always)
    get {
      return base.scalars
    }
  }

  @_inlineable
  public var scalarCount: Int32 {
    @inline(__always)
    get {
      return base.scalarCount
    }
  }

  @_inlineable
  public var handle: TensorHandle<Scalar> {
    @inline(__always)
    get {
      return base.handle
    }
  }

  @_inlineable
  public var shape: TensorShape {
    @inline(__always)
    get {
      return base.shape
    }
  }

  /// Creates a ${TensorXD} from a Tensor.
  /// - Note: This is an internal initializer and should not be exposed to
  ///   users.
  @_versioned
  @_inlineable @inline(__always)
  internal init(base: Tensor<Scalar>) {
    self.base = base
  }

  @_inlineable @inline(__always)
  public init(handle: TensorHandle<Scalar>) {
    self.init(base: Tensor(handle: handle))
  }

  /// Creates a ${TensorXD} from a Tensor. Returns nil if the Tensor does not
  /// have rank ${rank}.
  @_inlineable @inline(__always)
  public init?(_ other: Tensor<Scalar>) {
    guard other.rank == ${rank} else { return nil }
    self.init(base: other)
  }

  /// Creates a ${TensorXD} from a Tensor.
  /// - Precondition: The tensor must have rank ${rank}.
  @_inlineable @inline(__always)
  public init(identicallyRanked other: Tensor<Scalar>) {
    // NOTE: Precondition written using Assert op instead of Swift
    // `precondition` to prevent send/receive.
    // FIXME: Remove #if when Assert works with a GPU build.
#if false
    let rankTensor = Tensor<Int32>(${rank})
    let condition: Tensor<Bool> = other.rankTensor == rankTensor
    #tfop("Assert", condition, [rankTensor, other.rankTensor]) as Void
#endif
    self.init(base: other)
  }

  /// Creates a ${TensorXD} with the specified shape and contiguous scalars in
  /// row-major order.
  /// - Precondition: The shape must have ${rank} dimensions.
  /// - Precondition: The number of scalars must equal the product of the
  ///   dimensions of the shape.
  @_inlineable @inline(__always)
  public init(shape: TensorShape, scalars: [Scalar]) {
    // NOTE: Precondition written using Assert op instead of Swift
    // `precondition` to prevent send/receive.
    // TODO(danielzheng): Uncomment Assert op once receive bug is fixed here.
    // For some reason uncommenting even `rankTensor` below generates a receive.
    // let rankTensor = Tensor<Int32>(${rank})
    // let shapeTensor = Tensor<Int32>(shape.dimensions)
    // let condition: Tensor<Bool> = shapeTensor.scalarCountTensor == rankTensor
    // #tfop("Assert", condition, [rankTensor, shapeTensor]) as Void
    self.init(base: Tensor(shape: shape, scalars: scalars))
  }

  /// Creates a ${TensorXD} with the specified shape and a single, repeated
  /// value.
  /// - Precondition: The shape must have ${rank} dimensions.
  /// - Parameters:
  ///   - shape: The dimensions of the ${TensorXD}.
  ///   - repeatedValue: The scalar value to repeat.
  @_inlineable @inline(__always)
  public init(shape: TensorShape, repeating repeatedValue: Scalar) {
    // NOTE: Precondition written using Assert op instead of Swift
    // `precondition` to prevent send/receive.
    // FIXME: Remove #if when Assert works with a GPU build.
#if false
    let rankTensor = Tensor<Int32>(${rank})
    let shapeTensor = Tensor<Int32>(shape.dimensions)
    let condition: Tensor<Bool> = shapeTensor.scalarCountTensor == rankTensor
    #tfop("Assert", condition, [rankTensor, shapeTensor]) as Void
#endif
    self.init(base: Tensor(shape: shape, repeating: repeatedValue))
  }
}

/// Array literal support.
extension ${TensorXD} : ExpressibleByArrayLiteral {
  /// Creates a ${TensorXD} from with the specified elements.
  @_inlineable @inline(__always)
  public init(arrayLiteral elements: ${elementArrayLiteral}...) {
    self.init(elements)
  }
}

public extension ${TensorXD} where Scalar : Numeric {
  /// Perform an element-wise type conversion from a ${TensorXD}<T>.
  @_inlineable @inline(__always)
  init<FromType : Numeric>(_ other: ${TensorXD}<FromType>) {
    self.init(base: Tensor<Scalar>(other.base))
  }
}

/// Initializers
public extension ${TensorXD} {
  /// Creates a ${TensorXD} from a ShapedArray. Returns nil if the ShapedArray
  /// does not have rank ${rank}.
  @_inlineable @inline(__always)
  init?(_ other: ShapedArray<Scalar>) {
    self.init(Tensor(other))
  }

  /// Creates a ${TensorXD} from a ShapedArray.
  /// - Precondition: The ShapedArray must have rank ${rank}.
  @_inlineable @inline(__always)
  init(identicallyRanked other: ShapedArray<Scalar>) {
    self.init(identicallyRanked: Tensor(other))
  }

% if rank != 1:
  /// Creates a ${TensorXD} from an ${ArrayXD}.
  @_inlineable @inline(__always)
  init(_ other: ${ArrayXD}<Scalar>) {
    self.init(base: Tensor(other.base))
  }
% end

  /// Creates a ${TensorXD} containing the given element literals.
  @_inlineable @inline(__always)
  init(_ scalars: [${elementArrayLiteral}]) {
    self.init(base: Tensor<Scalar>(scalars))
  }

  /// Creates a ${TensorXD} containing the given element literals.
  @_inlineable @inline(__always)
  init(_ scalars: ${elementArrayLiteral}...) {
    self.init(scalars)
  }

  /// Creates a ${TensorXD} by broadcasting the given scalar to a {rank}-D
  /// shape with all dimensions being 1.
  @_inlineable @inline(__always)
  init(broadcasting scalar: Scalar) {
    self.init(shape: ${[1] * rank}, scalars: [scalar])
  }
}

public extension AccelerableByTensorFlow {
  /// Converts to a ${TensorXD} with all dimensions equal to 1.
  @_inlineable @inline(__always)
  func make${TensorXD}() -> ${TensorXD}<Self> {
    return ${TensorXD}(base: self.makeTensor(withRank: ${rank}))
  }
}

/// Memory transfer markers
/// - TODO: Remove when send/receive semantics gets revisited.
public extension ${TensorXD} {
  /// Indicate that this tensor is being moved to the accelerator.
  @_inlineable @inline(__always)
  func toDevice() -> ${TensorXD} {
    return ${TensorXD}(base: base.toDevice())
  }

  /// Indicate that this tensor is being moved to the host.
  @_inlineable @inline(__always)
  func toHost() -> ${TensorXD} {
    return ${TensorXD}(base: base.toHost())
  }
}

/// Factories
public extension ${TensorXD} where Scalar : Numeric {
  /// Creates a ${TensorXD} with all scalars set to zero.
  ///
  /// - Parameter shape: The dimensions of the ${TensorXD}.
  @_inlineable @inline(__always)
  init(zeros shape: TensorShape) {
    // NOTE: Precondition written using Assert op instead of Swift
    // `precondition` to prevent send/receive.
    // FIXME: Remove #if when Assert works with a GPU build.
#if false
    let rankTensor = Tensor<Int32>(${rank})
    let shapeTensor = Tensor<Int32>(shape.dimensions)
    let condition: Tensor<Bool> = shapeTensor.scalarCountTensor == rankTensor
    #tfop("Assert", condition, [rankTensor, shapeTensor]) as Void
#endif
    self.init(base: Tensor<Scalar>(zeros: shape))
  }

  /// Creates a ${TensorXD} with all scalars set to one.
  ///
  /// - Parameter shape: The dimensions of the ${TensorXD}.
  @_inlineable @inline(__always)
  init(ones shape: TensorShape) {
    // NOTE: Precondition written using Assert op instead of Swift
    // `precondition` to prevent send/receive.
    // FIXME: Remove #if when Assert works with a GPU build.
#if false
    let rankTensor = Tensor<Int32>(${rank})
    let shapeTensor = Tensor<Int32>(shape.dimensions)
    let condition: Tensor<Bool> = shapeTensor.scalarCountTensor == rankTensor
    #tfop("Assert", condition, [rankTensor, shapeTensor]) as Void
#endif
    self.init(base: Tensor<Scalar>(ones: shape))
  }
}

public extension ${TensorXD} where Scalar == Float {
  /// Creates a ${TensorXD} with the specified shape, randomly sampling
  /// scalar values from a normal distribution.
  ///
  /// - Parameters:
  ///   - shape: The dimensions of the ${TensorXD}.
  ///   - mean: The mean of the distribution.
  ///   - stddev: The standard deviation of the distribution.
  ///
  @_inlineable @inline(__always)
  init(randomNormal shape: TensorShape, mean: Scalar = 0, stddev: Scalar = 1) {
    precondition(shape.count == ${rank},
                 "The given shape doesn't match rank ${rank}")
    self.init(base: Tensor(randomNormal: shape, mean: mean, stddev: stddev))
  }

  /// Creates a tensor with the specified shape, randomly sampling scalar values
  /// from a uniform distribution.
  ///
  /// - Parameters:
  ///   - shape: The dimensions of the tensor.
  ///   - min: The lower bound of the distribution.
  ///   - max: The upper bound of the distribution.
  ///
  @_inlineable @inline(__always)
  init(randomUniform shape: TensorShape) {
    precondition(shape.count == ${rank},
                 "The given shape doesn't match rank ${rank}")
    self.init(base: Tensor(randomUniform: shape))
  }
}

public extension ${TensorXD} where Scalar == Double {
  /// Creates a ${TensorXD} with the specified shape, randomly sampling
  /// scalar values from a normal distribution.
  ///
  /// - Parameters:
  ///   - shape: The dimensions of the ${TensorXD}.
  ///   - mean: The mean of the distribution.
  ///   - stddev: The standard deviation of the distribution.
  ///
  @_inlineable @inline(__always)
  init(randomNormal shape: TensorShape, mean: Scalar = 0, stddev: Scalar = 1) {
    precondition(shape.count == ${rank},
                 "The given shape doesn't match rank ${rank}")
    self.init(base: Tensor(randomNormal: shape, mean: mean, stddev: stddev))
  }

  /// Creates a tensor with the specified shape, randomly sampling scalar values
  /// from a uniform distribution.
  ///
  /// - Parameters:
  ///   - shape: The dimensions of the tensor.
  ///   - min: The lower bound of the distribution.
  ///   - max: The upper bound of the distribution.
  ///
  @_inlineable @inline(__always)
  init(randomUniform shape: TensorShape) {
    precondition(shape.count == ${rank},
                 "The given shape doesn't match rank ${rank}")
    self.init(base: Tensor(randomUniform: shape))
  }
}

//===----------------------------------------------------------------------===//
// Automatic differentiation
//===----------------------------------------------------------------------===//

extension ${TensorXD} : DifferentiationArgument where Scalar : FloatingPoint {
  /// The currency type in the mathematical model of differentiation.
  public typealias DifferentiationCurrency = Scalar

  /// Creates an instance from the specified currency value and another,
  /// structurally isomorphic instance.
  ///
  /// - Parameters:
  ///   - value: The differentiation currency value for initializing the
  ///     instance.
  ///   - other: The other, structurally isomorphic instance.
  ///
  @_inlineable @inline(__always)
  public init(_ value: Scalar, isomorphicTo other: ${TensorXD}) {
    self.init(handle: #tfop("Fill", other.shapeTensor, value))
  }
}

//===----------------------------------------------------------------------===//
// Transforms
//===----------------------------------------------------------------------===//

public extension ${TensorXD} {
  /// Returns a transposed ${TensorXD}, with dimensions permuted in reverse
  /// order.
  @_inlineable @inline(__always)
  func transposed() -> ${TensorXD} {
    let defaultPermutations = rankTensor - 1 - Tensor<Int32>(
      rangeFrom: 0, to: rank, stride: 1
    )
    return transposed(withPermutations: defaultPermutations)
  }

  /// Return a copy of the ${TensorXD} collapsed into a Tensor1D in row-major
  /// order.
  @_inlineable @inline(__always)
  func flattened() -> Tensor1D<Scalar> {
    return Tensor1D(base: base.flattened())
  }
}

//===----------------------------------------------------------------------===//
// Indexing and slicing
//===----------------------------------------------------------------------===//

public extension ${TensorXD} {
% if rank == 1:
  @_inlineable
  /// Access the scalar specified by a given index.
  /// - Parameter index: Index of the scalar.
  subscript(index: Int32) -> Scalar {
    @inline(__always)
    get {
      return _TFGetScalarOrDie(base[index].handle)
    }
  }
% else:
  @_inlineable
  /// Access the element ${ElementTensor} specified by an index in the leading
  /// dimension.
  /// - Parameter index: Index of the element ${ElementTensor}.
  subscript(index: Int32) -> ${ElementTensor}<Scalar> {
    @inline(__always)
    get {
      return ${ElementTensor}<Scalar>(base: base[index])
    }
  }
% end

  /// Access the subtensor specified by a contiguous range of indices.
  /// - Parameter bounds: Contiguous range of indices.
  @_inlineable
  subscript(bounds: Range<Int32>) -> ${TensorXD} {
    @inline(__always)
    get {
      return ${TensorXD}<Scalar>(base: base[bounds])
    }
  }
}

//===----------------------------------------------------------------------===//
// Reduction
//===----------------------------------------------------------------------===//

public extension ${TensorXD} where Scalar : Numeric {
% if rank > 1:
  @_inlineable @inline(__always)
  func min(alongAxis axis: Int32) -> ${ElementTensor}<Scalar> {
    return ${ElementTensor}<Scalar>(base: base.min(alongAxes: [axis]))
  }

  @_inlineable @inline(__always)
  func max(alongAxis axis: Int32) -> ${ElementTensor}<Scalar> {
    return ${ElementTensor}<Scalar>(base: base.max(alongAxes: [axis]))
  }

  @_inlineable @inline(__always)
  func sum(alongAxis axis: Int32) -> ${ElementTensor}<Scalar> {
    return ${ElementTensor}<Scalar>(base: base.sum(alongAxes: [axis]))
  }

  @_inlineable @inline(__always)
  func mean(alongAxis axis: Int32) -> ${ElementTensor}<Scalar> {
    return ${ElementTensor}<Scalar>(base: base.mean(alongAxes: [axis]))
  }

  @_inlineable @inline(__always)
  func argmax(alongAxis axis: Int32) -> ${ElementTensor}<Int32> {
    return ${ElementTensor}<Int32>(base: base.argmax(alongAxis: axis))
  }

  @_inlineable @inline(__always)
  func argmin(alongAxis axis: Int32) -> ${ElementTensor}<Int32> {
    return ${ElementTensor}<Int32>(base: base.argmin(alongAxis: axis))
  }
% end
  @_inlineable @inline(__always)
  func argmax() -> Int32 {
    return _TFGetScalarOrDie(#tfop("ArgMax", flattened(), Tensor<Int32>(0),
                                   output_type: Int32.self))
  }

  @_inlineable @inline(__always)
  func argmin() -> Int32 {
    return _TFGetScalarOrDie(#tfop("ArgMin", flattened(), Tensor<Int32>(0),
                                   output_type: Int32.self))
  }
}

public extension ${TensorXD} {
% if rank < ranks[-1]:
  /// Returns a rank-lifted {LiftedTensor} with a leading dimension of 1.
  @_inlineable @inline(__always)
  func rankLifted() -> ${LiftedTensor}<Scalar> {
    return ${LiftedTensor}<Scalar>(base: base.rankLifted())
  }

  /// Returns a shape-expanded {LiftedTensor}, inserting a dimension of 1 at the
  /// specified shape index.
  @_inlineable @inline(__always)
  func expandingShape(at shapeIndex: Int32) -> ${LiftedTensor}<Scalar> {
    return ${LiftedTensor}<Scalar>(base: base.expandingShape(at: shapeIndex))
  }
% end
}

//===----------------------------------------------------------------------===//
// Description and visualization
//===----------------------------------------------------------------------===//

/// Make "print(some${TensorXD})" print a pretty form of the tensor.
extension ${TensorXD} : CustomStringConvertible {
  @_inlineable
  public var description: String {
    return base.description
  }
}

/// Make ${TensorXD}s show up nicely in the Xcode Playground results sidebar.
extension ${TensorXD} : CustomPlaygroundQuickLookable {
  @_inlineable
  public var customPlaygroundQuickLook: PlaygroundQuickLook {
    return .text(description)
  }
}

//===----------------------------------------------------------------------===//
// Array conversion
//===----------------------------------------------------------------------===//

public extension ${TensorXD} {
  @_inlineable
  var array: ${ArrayXD}<Scalar> {
    @inline(__always)
    get {
      // This is considered to be a well known way to produce a copy to the
      // host, so an "implicit copy to host" warning should not be produced.
% if rank == 1:
      return base.scalars
% else:
      return ${ArrayXD}(base: base.array)
% end
    }
  }
}
% end

//===----------------------------------------------------------------------===//
// Tensor conversion
//===----------------------------------------------------------------------===//

/// Value-preserving conversion initializer
public extension Tensor {
% for rank in ranks:
%   TensorXD = 'Tensor{}D'.format(rank)
  @_inlineable @inline(__always)
  init(_ tensor: ${TensorXD}<Scalar>) {
    self = tensor.base
  }
% end
}
